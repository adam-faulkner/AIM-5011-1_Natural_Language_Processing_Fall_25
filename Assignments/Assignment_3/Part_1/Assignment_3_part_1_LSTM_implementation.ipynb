{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBw__kO4P8-h"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KbGaKcUP8-j"
      },
      "source": [
        "LSTMs in Pytorch\n",
        "----------------\n",
        "\n",
        "Before getting to the example, note a few things. Pytorch\\'s LSTM\n",
        "expects all of its inputs to be 3D tensors. The semantics of the axes of\n",
        "these tensors is important. The first axis is the sequence itself, the\n",
        "second indexes instances in the mini-batch, and the third indexes\n",
        "elements of the input. We haven\\'t discussed mini-batching, so let\\'s\n",
        "just ignore that and assume we will always have just 1 dimension on the\n",
        "second axis. If we want to run the sequence model over the sentence\n",
        "\\\"The cow jumped\\\", our input should look like\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\begin{bmatrix}\n",
        "\\overbrace{q_\\text{The}}^\\text{row vector} \\\\\n",
        "q_\\text{cow} \\\\\n",
        "q_\\text{jumped}\n",
        "\\end{bmatrix}\n",
        "\\end{aligned}$$\n",
        "\n",
        "Except remember there is an additional 2nd dimension with size 1.\n",
        "\n",
        "In addition, you could go through the sequence one at a time, in which\n",
        "case the 1st axis will have size 1 also.\n",
        "\n",
        "Let\\'s see a quick example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c1NjwFwP8-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c52e6bb-54a4-4e5e-f937-34638e75b191"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d49bdcb1250>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmildQ69P8-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de33b125-4bc4-4741-a652-bf579171ace8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0187,  0.1713, -0.2944]],\n",
            "\n",
            "        [[-0.3521,  0.1026, -0.2971]],\n",
            "\n",
            "        [[-0.3191,  0.0781, -0.1957]],\n",
            "\n",
            "        [[-0.1634,  0.0941, -0.1637]],\n",
            "\n",
            "        [[-0.3368,  0.0959, -0.0538]]], grad_fn=<MkldnnRnnLayerBackward0>)\n",
            "(tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward0>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<StackBackward0>))\n"
          ]
        }
      ],
      "source": [
        "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
        "\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3))\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "\n",
        "# alternatively, we can do the entire sequence all at once.\n",
        "# the first value returned by LSTM is all of the hidden states throughout\n",
        "# the sequence. the second is just the most recent hidden state\n",
        "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
        "# The reason for this is that:\n",
        "# \"out\" will give you access to all hidden states in the sequence\n",
        "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
        "# by passing it as an argument  to the lstm at a later time\n",
        "# Add the extra 2nd dimension\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "print(hidden)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#make sure to upload 'imdb_small.csv' to the local directory\n",
        "imdb_dataset = pd.read_csv(\"imdb_small.csv\")\n",
        "#take a look at the data\n",
        "imdb_dataset.head()"
      ],
      "metadata": {
        "id": "_AUa14R0RDXY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "7ef5969c-1e9c-4f25-9dcc-94b6ff708159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                             review sentiment\n",
              "0           0  One of the other reviewers has mentioned that ...  positive\n",
              "1           1  A wonderful little production. <br /><br />The...  positive\n",
              "2           2  I thought this was a wonderful way to spend ti...  positive\n",
              "3           4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "4           5  Probably my all-time favorite movie, a story o...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac0757e9-47ed-4114-902c-a04280c12717\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac0757e9-47ed-4114-902c-a04280c12717')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac0757e9-47ed-4114-902c-a04280c12717 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac0757e9-47ed-4114-902c-a04280c12717');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a71a8a0e-dd62-4b20-9313-84e9fd11f907\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a71a8a0e-dd62-4b20-9313-84e9fd11f907')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a71a8a0e-dd62-4b20-9313-84e9fd11f907 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "imdb_dataset",
              "summary": "{\n  \"name\": \"imdb_dataset\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 288,\n        \"min\": 0,\n        \"max\": 1000,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          40,\n          448,\n          456\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"It had all the clich\\u00e9s of movies of this type and no substance. The plot went nowhere and at the end of the movie I felt like a sucker for watching it. The production was good; however, the script and acting were B-movie quality. The casting was poor because there were good actors mixed in with crumby actors. The good actors didn't hold their own nor did they lift up the others. <br /><br />This movie is not worthy of more words, but I will say more to meet the minimum requirement of ten lines. James Wood and Cuba Gooding, Jr. play caricatures of themselves in other movies. <br /><br />If you are looking for mindless entertainment, I still wouldn't recommend this movie.\",\n          \"I thrive on cinema....but there is a limit. A NAME isn't enough to make A MOVIE!. The beginning of the movie puts us in a mood to expect the unseen yet. But we remain hungry ( or angry..) till the end . Things are getting so confused that I admit that I DID NOT UNDERSTAND THE END or was there an end to this nonesense. The opportunity to make an outstanding movie was there but the target was totally missed. Next...\",\n          \"Oh, my goodness. I would have never thought it was possible for me to see a thriller worse than Domestic Disturbance this soon, but here it is. Armed with rotten plot, terrible editing, stilted acting, and headache-inducing 'style' (sorry, I have no other words for it), Sanctimony is the kind of movie that almost forces you to re-evaluate an entire genre; that is, this film is so bad that even the thrillers I condemned as complete failures now seem a little better.<br /><br />Now, not only Sanctimony is a terrible film in itself, it also succeeds in the difficult task of ripping off better movies and do a pathetic job with it. Right from the main titles -- nothing but a blatant attempt to reproduce the ones from Se7en -- I was under the impression that something didn't smell quite right. As soon as the movie started with a series of corny, wanna-be hip quick-cuts full of gory images and bombastic colors, I knew where that smell was coming from.<br /><br />It turns out that two policemen, or rather policeman Jim Renart (Michael Par\\u00e9) and policewoman Dorothy Smith (Jennifer Rubin), are investigating on a murder spree in Vancouver. A serial killer, known as \\\"Monkey Killer\\\" (what a menacing, chilling nickname, uh?) for his working methods, has killed quite a lot of people. You see, this nut apparently works following the proverb \\\"see no evil, hear no evil, speak no evil\\\" and cuts eyes, ears, and tongues out of his victims. So far, six eyes, six ears, and three tongues. In very ingenious fashion, Renart and Smith figure out that the Monkey Killer is probably going to kill other three people... well, because he probably wants to complete the number 666. So suddenly the film focuses on Tom Gerrick (Casper Van Dien), a young, successful, good-looking businessman, with a dreadful temper. And that's where the rip-off of American Psycho kicks in.<br /><br />So we follow the life of the two police officers and the young psychopath, none of which is interesting in the least, until they finally meet. Along the way to that, a disco where Renart barely misses Gerrick unintentionally offers us one of the funniest scenes in recent memory: Renart goes in the back of the disco club, because... well, just because the script tells us it's a suspect place; then, with one single punch in the stomach, Renard gets rid of a big guard who blocks the path, and the guard is never heard of again? Does this scene strike anyone else as completely unrealistic?<br /><br />Anyway, after another murder, Gerrick turns in as a witness, but Smith and especially Renart immediately suspect he might be the killer. In typical Basic Instinct fashion, Smith gets some dates with the young businessman, under the assumption that she might discover his true identity.<br /><br />I won't spoil the ending but it is, quite simply, an embarrassment; there are contradictions, some plot holes, issues that never get resolved, and especially there is one last scene where a brutal mass murder, supposed to be shocking and sad, comes off as such laughably overdone and nonsensical that I frankly can't imagine how anyone could not laugh at it.<br /><br />At 87 minutes, Sanctimony is really pushing it. You never care about one single character, because they are all so flat (not to mention boring) that you know exactly who is who the first time you meet them. You are never pulled into the story, because the scenes are connected through weak plot devices when not downright unnecessary and out of place. The acting ranges from average (Van Dien) to downright atrocious (Rubin, and most of the supporting cast); the music is abysmal generic techno, and the photography is one of the worst I have ever seen. Of course, like every fiasco of the genre, we are provided with a little bit of gratuitous nudity.<br /><br />3/10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Most students don't have access to GPUs so create a tiny version of the dataset that can fit on a CPU\n",
        "imdb_dataset = pd.concat([imdb_dataset[imdb_dataset.sentiment=='positive'].head(n=20),\n",
        "                          imdb_dataset[imdb_dataset.sentiment=='negative'].head(n=20)])\n"
      ],
      "metadata": {
        "id": "q-MKUrmb4qDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        return text, label"
      ],
      "metadata": {
        "id": "YlQlhDKbjKix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        " Remove Punctuation and get all the words from review dataset. Count all the words and sort it based on counts\n",
        "\n"
      ],
      "metadata": {
        "id": "Tlv9lXOvRbmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"This is a positive sentence.\", \"This is a negative sentence.\"]\n",
        "labels = [1, 0]\n",
        "\n",
        "dataset = TextDataset(texts, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=2)\n",
        "\n",
        "for batch in dataloader:\n",
        "    texts_batch, labels_batch = batch\n",
        "    print(texts_batch)\n",
        "    print(labels_batch)"
      ],
      "metadata": {
        "id": "pei6OajTjQTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a311f2-3e37-4c21-bd57-208f487f4bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('This is a positive sentence.', 'This is a negative sentence.')\n",
            "tensor([1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from collections import Counter\n",
        "all_reviews=list()\n",
        "for text in imdb_dataset.review.to_list():\n",
        "  text = text.lower()\n",
        "  text = \"\".join([ch for ch in text if ch not in punctuation])\n",
        "  all_reviews.append(text)\n",
        "all_text = \" \".join(all_reviews)\n",
        "all_words = all_text.split()\n",
        "\n",
        "# Count all the words using Counter Method\n",
        "count_words = Counter(all_words)\n",
        "total_words=len(all_words)\n",
        "sorted_words=count_words.most_common(total_words)\n",
        "print(f\"Top ten occuring words : {sorted_words[:10]}\")\n"
      ],
      "metadata": {
        "id": "1Ev-QYIORByG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11747c9-f5b4-49a8-a2c6-8860388d03a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top ten occuring words : [('the', 541), ('a', 260), ('of', 253), ('to', 200), ('and', 197), ('is', 164), ('in', 132), ('br', 114), ('i', 108), ('it', 106)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        " Create a dictionary to convert words to Integers based on the number of occurrence of the word"
      ],
      "metadata": {
        "id": "2PM2wDxESAwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "we will start creating dictionary with index 1 because 0 is reserved for padding\n",
        "'''\n",
        "\n",
        "vocab_to_int={w:i+1 for i,(w,c) in enumerate(sorted_words)}\n",
        "print(vocab_to_int)"
      ],
      "metadata": {
        "id": "yMcFeNbtR_9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9355e130-b3ff-4ab2-b48c-7c3fc488b6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'a': 2, 'of': 3, 'to': 4, 'and': 5, 'is': 6, 'in': 7, 'br': 8, 'i': 9, 'it': 10, 'this': 11, 'that': 12, 'was': 13, 'but': 14, 'movie': 15, 'with': 16, 'as': 17, 'for': 18, 'film': 19, 'not': 20, 'on': 21, 'one': 22, 'its': 23, 'you': 24, 'all': 25, 'at': 26, 'are': 27, 'by': 28, 'be': 29, 'have': 30, 'his': 31, 'so': 32, 'like': 33, 'from': 34, 'or': 35, 'just': 36, 'an': 37, 'what': 38, 'if': 39, 'who': 40, 'about': 41, 'even': 42, 'some': 43, 'out': 44, 'only': 45, 'he': 46, 'no': 47, 'has': 48, 'more': 49, 'when': 50, 'my': 51, 'they': 52, 'first': 53, 'me': 54, 'very': 55, 'see': 56, 'there': 57, 'we': 58, 'story': 59, 'would': 60, 'than': 61, 'been': 62, 'up': 63, 'much': 64, 'which': 65, 'into': 66, 'time': 67, 'most': 68, 'way': 69, 'will': 70, 'bad': 71, 'because': 72, 'go': 73, 'never': 74, 'far': 75, 'how': 76, 'good': 77, 'war': 78, 'show': 79, 'say': 80, 'little': 81, 'then': 82, 'scenes': 83, 'where': 84, 'really': 85, 'love': 86, 'do': 87, 'people': 88, 'another': 89, 'them': 90, 'were': 91, 'least': 92, 'other': 93, 'pretty': 94, 'their': 95, 'too': 96, 'ive': 97, 'films': 98, 'movies': 99, 'end': 100, 'thing': 101, 'life': 102, 'many': 103, 'her': 104, 'young': 105, 'best': 106, 'acting': 107, 'any': 108, 'three': 109, 'does': 110, 'could': 111, 'though': 112, 'right': 113, 'may': 114, 'your': 115, 'down': 116, 'great': 117, 'characters': 118, 'watch': 119, 'these': 120, 'find': 121, 'few': 122, 'man': 123, 'character': 124, 'also': 125, 'those': 126, 'however': 127, 'cant': 128, 'better': 129, 'him': 130, 'made': 131, 'such': 132, 'police': 133, 'got': 134, 'get': 135, 'well': 136, 'can': 137, 'truly': 138, 'comes': 139, 'while': 140, 'still': 141, 'us': 142, 'same': 143, 'back': 144, 'hard': 145, 'mountain': 146, 'dont': 147, 'going': 148, 'played': 149, 'beautiful': 150, 'watching': 151, 'violence': 152, 'actors': 153, 'thought': 154, 'killer': 155, 'interesting': 156, 'director': 157, 'world': 158, 'big': 159, 'make': 160, 'seen': 161, 'had': 162, 'must': 163, 'im': 164, 'itself': 165, 'cinema': 166, 'actually': 167, 'bit': 168, 'something': 169, 'thats': 170, 'true': 171, 'terrible': 172, 'through': 173, 'scene': 174, 'after': 175, 'use': 176, 'home': 177, 'goes': 178, 'around': 179, 'saw': 180, 'watched': 181, 'rather': 182, 'plot': 183, 'serial': 184, 'point': 185, 'know': 186, 'own': 187, 'gets': 188, 'cast': 189, 'work': 190, 'times': 191, 'fun': 192, 'original': 193, 'simply': 194, 'off': 195, 'again': 196, 'especially': 197, 'someone': 198, 'top': 199, 'theres': 200, 'lot': 201, 'cold': 202, 'real': 203, 'titta': 204, 'oz': 205, 'forget': 206, 'ever': 207, 'being': 208, 'gives': 209, 'things': 210, 'years': 211, 'she': 212, 'money': 213, 'mr': 214, 'human': 215, 'seems': 216, 'action': 217, 'look': 218, 'taken': 219, 'under': 220, 'rest': 221, 'old': 222, 'having': 223, 'makes': 224, 'series': 225, 'nice': 226, 'let': 227, 'did': 228, 'enjoy': 229, 'now': 230, 'wrong': 231, 'full': 232, 'girl': 233, 'cell': 234, 'mind': 235, 'nothing': 236, 'enough': 237, 'audience': 238, 'hollywood': 239, 'against': 240, 'beginning': 241, 'before': 242, 'murder': 243, 'since': 244, 'here': 245, 'di': 246, 'ridiculous': 247, 'funny': 248, 'awful': 249, 'boll': 250, 'carver': 251, 'whole': 252, 'bmovie': 253, 'exactly': 254, 'set': 255, 'death': 256, 'fact': 257, 'lack': 258, 'become': 259, 'wonderful': 260, 'comedy': 261, 'our': 262, 'every': 263, 'air': 264, 'success': 265, 'different': 266, 'new': 267, 'person': 268, 'place': 269, 'direction': 270, 'boring': 271, 'performance': 272, 'sure': 273, 'wanna': 274, 'always': 275, 'version': 276, 'think': 277, 'band': 278, 'hes': 279, 'takes': 280, 'heard': 281, 'karen': 282, 'fine': 283, 'role': 284, 'tell': 285, 'looks': 286, 'kind': 287, 'over': 288, 'writing': 289, 'weird': 290, 'inman': 291, 'ada': 292, 'romantic': 293, 'worse': 294, 'between': 295, 'drama': 296, 'ryan': 297, 'why': 298, 'based': 299, 'beyond': 300, 'mumbai': 301, 'anand': 302, 'family': 303, 'towards': 304, 'house': 305, 'halfway': 306, 'apparently': 307, 'half': 308, 'le': 309, 'conseguenze': 310, 'dellamore': 311, 'almost': 312, 'during': 313, 'jake': 314, 'parents': 315, 'give': 316, 'worst': 317, 'everything': 318, 'star': 319, 'cry': 320, 'wasnt': 321, 'classic': 322, 'called': 323, 'given': 324, 'city': 325, 'prison': 326, 'high': 327, 'main': 328, 'due': 329, 'wouldnt': 330, 'nasty': 331, 'kill': 332, 'away': 333, 'turned': 334, 'production': 335, 'piece': 336, 'michael': 337, 'worth': 338, 'dream': 339, 'done': 340, 'dialogue': 341, '2': 342, 'portrait': 343, 'relations': 344, 'play': 345, 'theme': 346, 'live': 347, 'cause': 348, 'brings': 349, 'kids': 350, 'children': 351, 'slow': 352, 'today': 353, 'bring': 354, 'kid': 355, 'tv': 356, 'need': 357, 'change': 358, 'view': 359, 'believe': 360, '10': 361, 'lets': 362, 'itbr': 363, 'fantastic': 364, 'fan': 365, 'soundtrack': 366, 'constant': 367, 'fails': 368, 'feel': 369, 'major': 370, 'lost': 371, 'earlier': 372, 'remember': 373, 'dark': 374, 'took': 375, 'actor': 376, '20': 377, 'miles': 378, 'die': 379, 'hal': 380, 'painful': 381, 'mention': 382, 'yes': 383, 'looking': 384, 'carpenter': 385, 'giving': 386, 'actress': 387, 'sort': 388, 'several': 389, 'year': 390, 'imagination': 391, 'try': 392, 'trying': 393, 'matter': 394, 'instead': 395, 'long': 396, 'onto': 397, 'take': 398, 'nominated': 399, 'walk': 400, 'sequences': 401, 'once': 402, 'screen': 403, 'quite': 404, 'becomes': 405, 'southern': 406, 'left': 407, 'behind': 408, 'indeed': 409, 'scenario': 410, 'coming': 411, 'perhaps': 412, 'two': 413, 'unfolds': 414, 'surprisingly': 415, 'performances': 416, 'thriller': 417, 'robert': 418, 'script': 419, '70s': 420, 'system': 421, 'approach': 422, 'cannot': 423, 'turning': 424, 'part': 425, 'italy': 426, 'hotel': 427, 'ten': 428, 'girolamos': 429, 'bluff': 430, 'eventually': 431, 'consequences': 432, 'reason': 433, 'unsympathetic': 434, 'emotional': 435, 'david': 436, 'amount': 437, 'timebr': 438, 'looked': 439, 'playing': 440, 'lead': 441, 'totally': 442, 'similar': 443, 'idea': 444, 'waste': 445, 'cheap': 446, 'making': 447, 'didnt': 448, 'monster': 449, 'minutes': 450, 'shakespeare': 451, 'bizarre': 452, 'horrible': 453, 'machine': 454, 'final': 455, 'spider': 456, 'victim': 457, 'both': 458, 'crappy': 459, 'honestly': 460, 'blockbuster': 461, 'video': 462, 'suicide': 463, 'mentioned': 464, 'episode': 465, 'struck': 466, 'drugs': 467, 'sex': 468, 'face': 469, 'agenda': 470, 'irish': 471, 'appeal': 472, 'shows': 473, 'dare': 474, 'doesnt': 475, 'ready': 476, 'wholl': 477, 'inmates': 478, 'skills': 479, 'experience': 480, 'touch': 481, 'sense': 482, 'realism': 483, 'entire': 484, 'editing': 485, 'plays': 486, 'particularly': 487, 'spend': 488, 'sitting': 489, 'theater': 490, 'disappointed': 491, 'realize': 492, 'style': 493, 'id': 494, 'managed': 495, 'sexy': 496, 'matteis': 497, 'stunning': 498, 'mattei': 499, 'offers': 500, 'vivid': 501, 'each': 502, 'next': 503, 'picture': 504, 'loneliness': 505, 'sincere': 506, 'steve': 507, 'talented': 508, 'come': 509, 'probably': 510, 'favorite': 511, '15': 512, 'last': 513, '25': 514, 'paul': 515, 'eyes': 516, 'sympathetic': 517, 'roles': 518, 'says': 519, 'whats': 520, 'happening': 521, 'believable': 522, 'theyd': 523, 'seahunt': 524, 'black': 525, 'white': 526, 'sea': 527, 'pace': 528, 'read': 529, 'points': 530, 'lines': 531, 'hell': 532, 'mom': 533, 'liked': 534, 'famous': 535, 'george': 536, 'everybody': 537, 'should': 538, 'terror': 539, 'title': 540, 'excellent': 541, 'exception': 542, 'dad': 543, 'brother': 544, 'newbury': 545, 'recall': 546, 'tigers': 547, 'lots': 548, 'anyone': 549, 'knows': 550, 'dvd': 551, 'shame': 552, 'others': 553, 'guy': 554, 'cliffhanger': 555, 'rescue': 556, 'shoot': 557, 'entertaining': 558, 'delivers': 559, 'plenty': 560, 'unintentionally': 561, 'john': 562, 'tick': 563, 'constantly': 564, 'anybody': 565, 'whilst': 566, 'using': 567, 'help': 568, 'needed': 569, 'cares': 570, 'expectations': 571, 'turn': 572, 'youre': 573, 'wearing': 574, 'absolutely': 575, 'wont': 576, 'although': 577, 'music': 578, 'start': 579, 'singer': 580, 'complex': 581, 'accurate': 582, 'facts': 583, 'stronger': 584, 'job': 585, 'included': 586, 'unfortunately': 587, 'werent': 588, 'door': 589, 'tarsem': 590, 'want': 591, 'talk': 592, 'inside': 593, 'genre': 594, '90s': 595, 'failed': 596, 'physically': 597, 'less': 598, 'jennifer': 599, 'brain': 600, 'guess': 601, 'ventures': 602, 'inbr': 603, 'noticed': 604, 'maybe': 605, 'contrast': 606, 'cinematography': 607, 'costumes': 608, 'else': 609, 'am': 610, 'along': 611, 'compelling': 612, 'silver': 613, 'civil': 614, 'starring': 615, 'jude': 616, 'law': 617, 'nicole': 618, 'kidman': 619, 'renée': 620, 'zellweger': 621, 'filmbr': 622, 'entirely': 623, 'battle': 624, 'sequence': 625, 'edward': 626, 'soldier': 627, 'gruesome': 628, 'north': 629, 'carolina': 630, 'appears': 631, 'interest': 632, 'root': 633, 'soldiers': 634, 'soon': 635, 'hero': 636, 'incidentally': 637, 'wanting': 638, 'meanwhile': 639, 'farm': 640, 'adas': 641, 'course': 642, 'helps': 643, 'together': 644, 'cope': 645, 'brought': 646, 'upon': 647, 'within': 648, 'settings': 649, 'whom': 650, 'father': 651, 'ray': 652, 'deeply': 653, 'mother': 654, 'affected': 655, 'changed': 656, 'mostly': 657, 'message': 658, 'shots': 659, 'land': 660, 'past': 661, 'century': 662, 'atmosphere': 663, 'absurd': 664, 'nature': 665, 'relationship': 666, 'mistake': 667, 'romance': 668, 'nor': 669, 'unique': 670, 'torn': 671, 'altogether': 672, 'crossfire': 673, 'hanging': 674, 'bars': 675, 'drunk': 676, 'happens': 677, 'mitchum': 678, 'whos': 679, 'ryans': 680, 'second': 681, 'third': 682, 'dmytryk': 683, 'small': 684, 'died': 685, 'stretching': 686, 'attempt': 687, 'dealt': 688, 'news': 689, 'felt': 690, 'antisemitism': 691, 'fit': 692, 'odd': 693, 'discreet': 694, 'ardh': 695, 'satya': 696, 'govind': 697, 'nihalani': 698, 'leading': 699, 'tells': 700, 'unlike': 701, 'practical': 702, 'velankar': 703, 'cop': 704, 'poor': 705, 'fathers': 706, 'anands': 707, 'crime': 708, 'frustrations': 709, 'spirit': 710, 'finally': 711, 'memorable': 712, 'followed': 713, 'political': 714, 'aspects': 715, 'girolamo': 716, 'originally': 717, 'introduced': 718, 'nonlife': 719, 'yet': 720, 'bank': 721, 'usual': 722, 'spectators': 723, 'without': 724, 'caught': 725, 'appearing': 726, 'truth': 727, 'ultimately': 728, 'troubles': 729, 'spectator': 730, 'decides': 731, 'concern': 732, 'reveal': 733, 'irrelevant': 734, 'seemed': 735, 'innovative': 736, 'existence': 737, 'unexpectedly': 738, 'feeling': 739, 'edited': 740, 'scottish': 741, 'limbalsamatore': 742, 'insomnia': 743, 'provide': 744, 'renaissance': 745, 'decline': 746, 'week': 747, 'certainly': 748, 'created': 749, 'season': 750, 'china': 751, 'hong': 752, 'kong': 753, 'stereotypical': 754, 'song': 755, 'lovers': 756, 'genuine': 757, 'moments': 758, 'closet': 759, 'fighting': 760, 'decide': 761, 'brilliant': 762, 'comments': 763, 'forward': 764, 'storyline': 765, 'lame': 766, 'happy': 767, 'keitel': 768, 'effort': 769, 'oddness': 770, 'jokes': 771, 'problem': 772, '12': 773, 'came': 774, 'eating': 775, 'men': 776, 'b': 777, 'formula': 778, 'usually': 779, 'daughter': 780, 'predictable': 781, 'bolls': 782, 'enjoyed': 783, 'bought': 784, 'research': 785, 'island': 786, 'warned': 787, 'names': 788, 'til': 789, 'schweiger': 790, 'udo': 791, 'kier': 792, 'gms': 793, 'reminds': 794, 'havent': 795, 'until': 796, 'annoying': 797, 'appreciate': 798, 'tried': 799, 'write': 800, 'english': 801, 'keep': 802, 'unbelievable': 803, 'school': 804, 'supposed': 805, 'town': 806, 'fight': 807, 'chance': 808, 'development': 809, 'except': 810, 'stolen': 811, 'wars': 812, 'kings': 813, 'nazis': 814, 'juvenile': 815, 'save': 816, 'theyve': 817, 'recognition': 818, 'hours': 819, 'utterly': 820, 'bmovies': 821, 'anime': 822, 'speed': 823, 'racer': 824, 'hilarious': 825, 'godzilla': 826, 'fx': 827, 'animation': 828, 'run': 829, 'barely': 830, '310': 831, 'counting': 832, 'unless': 833, 'shes': 834, 'critique': 835, 'stand': 836, 'hated': 837, 'drummer': 838, 'tedious': 839, 'london': 840, 'miniskirt': 841, 'restricted': 842, 'sticker': 843, 'age': 844, 'panties': 845, 'pg13': 846, 'rated': 847, 'rent': 848, 'suicides': 849, 'teenagers': 850, 'hopper': 851, 'killings': 852, 'begin': 853, 'struggle': 854, 'reviewers': 855, '1': 856, 'youll': 857, 'hooked': 858, 'happened': 859, 'mebr': 860, 'brutality': 861, 'unflinching': 862, 'word': 863, 'trust': 864, 'faint': 865, 'hearted': 866, 'timid': 867, 'pulls': 868, 'punches': 869, 'regards': 870, 'hardcore': 871, 'wordbr': 872, 'nickname': 873, 'oswald': 874, 'maximum': 875, 'security': 876, 'state': 877, 'penitentary': 878, 'focuses': 879, 'mainly': 880, 'emerald': 881, 'experimental': 882, 'section': 883, 'cells': 884, 'glass': 885, 'fronts': 886, 'inwards': 887, 'privacy': 888, 'em': 889, 'manyaryans': 890, 'muslims': 891, 'gangstas': 892, 'latinos': 893, 'christians': 894, 'italians': 895, 'moreso': 896, 'scuffles': 897, 'stares': 898, 'dodgy': 899, 'dealings': 900, 'shady': 901, 'agreements': 902, 'awaybr': 903, 'pictures': 904, 'painted': 905, 'mainstream': 906, 'audiences': 907, 'charm': 908, 'romanceoz': 909, 'mess': 910, 'surreal': 911, 'couldnt': 912, 'developed': 913, 'taste': 914, 'accustomed': 915, 'levels': 916, 'graphic': 917, 'injustice': 918, 'crooked': 919, 'guards': 920, 'sold': 921, 'nickel': 922, 'order': 923, 'mannered': 924, 'middle': 925, 'class': 926, 'bitches': 927, 'street': 928, 'comfortable': 929, 'uncomfortable': 930, 'viewingthats': 931, 'darker': 932, 'side': 933, 'filming': 934, 'technique': 935, 'unassuming': 936, 'oldtimebbc': 937, 'fashion': 938, 'comforting': 939, 'sometimes': 940, 'discomforting': 941, 'extremely': 942, 'chosen': 943, 'sheen': 944, 'polari': 945, 'voices': 946, 'pat': 947, 'seamless': 948, 'guided': 949, 'references': 950, 'williams': 951, 'diary': 952, 'entries': 953, 'terrificly': 954, 'written': 955, 'performed': 956, 'masterful': 957, 'masters': 958, 'fantasy': 959, 'guard': 960, 'traditional': 961, 'techniques': 962, 'remains': 963, 'solid': 964, 'disappears': 965, 'knowledge': 966, 'senses': 967, 'concerning': 968, 'orton': 969, 'halliwell': 970, 'sets': 971, 'flat': 972, 'halliwells': 973, 'murals': 974, 'decorating': 975, 'surface': 976, 'terribly': 977, 'hot': 978, 'summer': 979, 'weekend': 980, 'conditioned': 981, 'lighthearted': 982, 'simplistic': 983, 'witty': 984, 'likable': 985, 'bread': 986, 'suspected': 987, 'match': 988, 'risk': 989, 'addiction': 990, 'proof': 991, 'woody': 992, 'allen': 993, 'fully': 994, 'control': 995, 'grown': 996, 'lovebr': 997, 'laughed': 998, 'woodys': 999, 'comedies': 1000, 'decade': 1001, 'impressed': 1002, 'scarlet': 1003, 'johanson': 1004, 'tone': 1005, 'image': 1006, 'jumped': 1007, 'average': 1008, 'spirited': 1009, 'womanbr': 1010, 'crown': 1011, 'jewel': 1012, 'career': 1013, 'wittier': 1014, 'devil': 1015, 'wears': 1016, 'prada': 1017, 'superman': 1018, 'friends': 1019, 'petter': 1020, 'visually': 1021, 'telling': 1022, 'power': 1023, 'situations': 1024, 'encounter': 1025, 'variation': 1026, 'arthur': 1027, 'schnitzlers': 1028, 'transfers': 1029, 'present': 1030, 'york': 1031, 'meet': 1032, 'connect': 1033, 'connected': 1034, 'previous': 1035, 'contact': 1036, 'stylishly': 1037, 'sophisticated': 1038, 'luxurious': 1039, 'habitatbr': 1040, 'souls': 1041, 'stages': 1042, 'inhabits': 1043, 'fulfillment': 1044, 'discerns': 1045, 'case': 1046, 'encounterbr': 1047, 'buscemi': 1048, 'rosario': 1049, 'dawson': 1050, 'carol': 1051, 'kane': 1052, 'imperioli': 1053, 'adrian': 1054, 'grenier': 1055, 'alivebr': 1056, 'wish': 1057, 'luck': 1058, 'await': 1059, 'anxiously': 1060, 'alltime': 1061, 'selflessness': 1062, 'sacrifice': 1063, 'dedication': 1064, 'noble': 1065, 'preachy': 1066, 'despite': 1067, 'lukas': 1068, 'tears': 1069, 'bette': 1070, 'davis': 1071, 'delight': 1072, 'grandma': 1073, 'dressedup': 1074, 'midgets': 1075, 'mothers': 1076, 'awakening': 1077, 'roof': 1078, 'startling': 1079, 'dozen': 1080, 'thumbs': 1081, 'resurrection': 1082, 'dated': 1083, 'tech': 1084, 'excitement': 1085, 'mei': 1086, 'grew': 1087, 'gunsmoke': 1088, 'heros': 1089, 'weekyou': 1090, 'vote': 1091, 'comeback': 1092, 'huntwe': 1093, 'water': 1094, 'adventureoh': 1095, 'thank': 1096, 'outlet': 1097, 'viewpoints': 1098, 'moviesso': 1099, 'ole': 1100, 'saywould': 1101, 'plus': 1102, 'huntif': 1103, 'rhymes': 1104, 'submitor': 1105, 'leave': 1106, 'doubt': 1107, 'quitif': 1108, 'gut': 1109, 'wrenching': 1110, 'laughter': 1111, 'camp': 1112, 'prisoners': 1113, 'clooney': 1114, 'roll': 1115, 'sorrow': 1116, 'recommand': 1117, 'greetings': 1118, 'bart': 1119, 'remade': 1120, 'capture': 1121, 'flavor': 1122, '1963': 1123, 'liam': 1124, 'neeson': 1125, 'holds': 1126, 'owen': 1127, 'wilson': 1128, 'luke': 1129, 'fault': 1130, 'strayed': 1131, 'shirley': 1132, 'jackson': 1133, 'attempts': 1134, 'grandiose': 1135, 'thrill': 1136, 'trade': 1137, 'snazzier': 1138, 'special': 1139, 'effects': 1140, 'friction': 1141, 'older': 1142, 'filmit': 1143, 'places': 1144, 'nervous': 1145, '7475': 1146, 'sister': 1147, 'berkshire': 1148, 'england': 1149, 'snow': 1150, 'appearance': 1151, 'grizzly': 1152, 'adams': 1153, 'dan': 1154, 'haggery': 1155, 'shot': 1156, 'dies': 1157, 'etc': 1158, 'please': 1159, 'knowthe': 1160, 'fitness': 1161, 'club': 1162, 'nearest': 1163, 'hear': 1164, 'sequels': 1165, 'surprise': 1166, '1990s': 1167, 'glut': 1168, 'cashed': 1169, 'concept': 1170, 'sly': 1171, 'stop': 1172, 'stallones': 1173, 'careerbr': 1174, 'nitpickers': 1175, 'expert': 1176, 'climbing': 1177, 'basejumping': 1178, 'aviation': 1179, 'facial': 1180, 'expressions': 1181, 'excuses': 1182, 'dismiss': 1183, 'overblown': 1184, 'pile': 1185, 'junk': 1186, 'stallone': 1187, 'outacted': 1188, 'horse': 1189, 'nonsense': 1190, 'lovable': 1191, 'undeniably': 1192, 'romp': 1193, 'thrills': 1194, 'laughsbr': 1195, 'youve': 1196, 'lithgows': 1197, 'sneery': 1198, 'evilness': 1199, 'box': 1200, 'baddies': 1201, 'permanently': 1202, 'harassed': 1203, 'hapless': 1204, 'turncoat': 1205, 'agent': 1206, 'rex': 1207, 'linn': 1208, 'traversbr': 1209, 'henry': 1210, 'rooker': 1211, 'noteworthy': 1212, 'cringeworthy': 1213, 'insists': 1214, 'shrieking': 1215, 'disbelief': 1216, 'captors': 1217, 'hurt': 1218, 'surely': 1219, 'ralph': 1220, 'waites': 1221, 'frank': 1222, 'grinning': 1223, 'plummets': 1224, 'deathbr': 1225, 'former': 1226, 'londons': 1227, 'burning': 1228, 'craig': 1229, 'fairbrass': 1230, 'brit': 1231, 'cropper': 1232, 'football': 1233, 'kickingbr': 1234, 'judgement': 1235, 'happen': 1236, 'lower': 1237, 'volume': 1238, 'qaulen': 1239, 'helicopter': 1240, 'hrs': 1241, 'regret': 1242, 'rajnikanth': 1243, 'carries': 1244, 'shoulders': 1245, 'isnt': 1246, 'anything': 1247, 'arrehman': 1248, 'grow': 1249, 'liking': 1250, 'carpenters': 1251, 'detailsbr': 1252, 'cynthia': 1253, 'gibb': 1254, 'portrays': 1255, 'election': 1256, 'naive': 1257, 'dumb': 1258, 'personalitybr': 1259, 'louise': 1260, 'fletcher': 1261, 'agnes': 1262, 'terrific': 1263, 'karens': 1264, 'motherbr': 1265, 'songs': 1266, 'album': 1267, 'ratings': 1268, 'usa': 1269, 'countries': 1270, 'exotic': 1271, 'masterpiece': 1272, 'dizzying': 1273, 'trip': 1274, 'vast': 1275, 'conclusive': 1276, 'evidence': 1277, 'achieved': 1278, 'beings': 1279, 'unleash': 1280, 'uninhibited': 1281, 'imaginations': 1282, 'boldness': 1283, 'pushing': 1284, 'aside': 1285, 'thoughts': 1286, 'fall': 1287, 'formulas': 1288, 'cliches': 1289, 'creating': 1290, 'magnificent': 1291, 'datebr': 1292, 'numerous': 1293, 'complaints': 1294, 'anywhere': 1295, 'substance': 1296, 'poorly': 1297, 'negatively': 1298, 'criticize': 1299, 'miss': 1300, 'landmark': 1301, 'tradition': 1302, 'future': 1303, 'hopefully': 1304, 'follow': 1305, 'opened': 1306, 'slam': 1307, 'singh': 1308, 'personally': 1309, 'welcome': 1310, 'challenge': 1311, 'himbr': 1312, 'weve': 1313, 'agree': 1314, 'overworked': 1315, 'depict': 1316, 'killers': 1317, 'worked': 1318, 'blaze': 1319, 'trail': 1320, 'twist': 1321, 'transported': 1322, 'presented': 1323, 'fascinating': 1324, 'journey': 1325, 'mysterious': 1326, 'subject': 1327, 'studiedbr': 1328, 'bog': 1329, 'scientific': 1330, 'jargon': 1331, 'explain': 1332, 'lopez': 1333, 'enter': 1334, 'lies': 1335, 'laboratory': 1336, 'table': 1337, 'wrapped': 1338, 'twizzlers': 1339, 'jaunted': 1340, 'entity': 1341, 'wants': 1342, 'explanations': 1343, 'ground': 1344, 'desires': 1345, 'showed': 1346, 'reality': 1347, 'bright': 1348, 'visuals': 1349, 'nonetheless': 1350, 'design': 1351, 'astonishing': 1352, 'surprised': 1353, 'oscars': 1354, 'itd': 1355, 'picturebr': 1356, 'repeating': 1357, 'myself': 1358, 'stress': 1359, 'open': 1360, 'wonders': 1361, 'eyepopping': 1362, 'feast': 1363, 'assured': 1364, 'crazy': 1365, 'psychology': 1366, 'alley': 1367, 'leaving': 1368, 'member': 1369, 'whoever': 1370, 'smokingbr': 1371, '4': 1372, 'redone': 1373, 'clichéd': 1374, 'rehashed': 1375, 'overthetop': 1376, 'seem': 1377, 'unavoidable': 1378, 'conflict': 1379, 'dealing': 1380, 'largescale': 1381, 'combat': 1382, 'grain': 1383, 'warera': 1384, 'calling': 1385, 'opens': 1386, 'literally': 1387, 'quickanddirty': 1388, 'puts': 1389, 'glory': 1390, 'zwick': 1391, 'period': 1392, 'centers': 1393, 'disgruntled': 1394, 'confederate': 1395, 'disgusted': 1396, 'homesick': 1397, 'hamlet': 1398, 'equally': 1399, 'belle': 1400, 'monroe': 1401, 'glance': 1402, 'setup': 1403, 'formulaic': 1404, 'sympathy': 1405, 'reluctant': 1406, 'tribulations': 1407, 'battlefield': 1408, 'segments': 1409, 'relatively': 1410, 'unimpressive': 1411, 'somewhat': 1412, 'contrivedbr': 1413, 'drastic': 1414, 'intrepid': 1415, 'turns': 1416, 'deserter': 1417, 'saving': 1418, 'potentially': 1419, 'confusing': 1420, 'confederates': 1421, 'begins': 1422, 'odyssey': 1423, 'homeward': 1424, 'cultured': 1425, 'ways': 1426, 'prove': 1427, 'fields': 1428, 'transformed': 1429, 'wilderbeast': 1430, 'toughasnails': 1431, 'ruby': 1432, 'thewes': 1433, 'put': 1434, 'importantly': 1435, 'isolation': 1436, 'adabr': 1437, 'disturbing': 1438, 'wartorn': 1439, 'south': 1440, 'interact': 1441, 'enhanced': 1442, 'brendan': 1443, 'gleeson': 1444, 'rubys': 1445, 'deadbeat': 1446, 'winstone': 1447, 'unrepentant': 1448, 'lawman': 1449, 'natalie': 1450, 'portman': 1451, 'troubled': 1452, 'isolated': 1453, 'greatly': 1454, 'northern': 1455, 'aggression': 1456, 'pervading': 1457, 'antiwar': 1458, 'accented': 1459, 'effective': 1460, 'haunting': 1461, 'score': 1462, 'chillingly': 1463, 'virginia': 1464, 'communicated': 1465, 'scarred': 1466, 'traumatized': 1467, 'fought': 1468, 'weapons': 1469, 'tactics': 1470, 'hellish': 1471, 'effect': 1472, 'timelessly': 1473, 'relevantbr': 1474, 'anthony': 1475, 'minghella': 1476, 'manages': 1477, 'maintain': 1478, 'gloomy': 1479, 'mood': 1480, 'denigrated': 1481, 'tepid': 1482, 'climax': 1483, 'justice': 1484, 'wonderfully': 1485, 'formed': 1486, 'awkwardly': 1487, 'tacked': 1488, 'inherently': 1489, 'distant': 1490, 'abstracted': 1491, 'fits': 1492, 'dismal': 1493, 'plotbr': 1494, 'neither': 1495, 'traits': 1496, 'feelgood': 1497, 'inspiring': 1498, 'vision': 1499, 'era': 1500, 'entertain': 1501, 'absorb': 1502, 'lives': 1503, 'apart': 1504, 'desperate': 1505, 'rid': 1506, 'repercussions': 1507, 'taut': 1508, 'organically': 1509, 'gripping': 1510, 'dmytryks': 1511, 'distinctive': 1512, 'suspense': 1513, 'unlikely': 1514, 'devices': 1515, 'noir': 1516, 'cyclebr': 1517, 'bivouacked': 1518, 'washington': 1519, 'dc': 1520, 'company': 1521, 'restlessness': 1522, 'strangers': 1523, 'apartment': 1524, 'belligerent': 1525, 'beats': 1526, 'host': 1527, 'sam': 1528, 'levene': 1529, 'jewish': 1530, 'detective': 1531, 'investigates': 1532, 'assigned': 1533, 'outfit': 1534, 'suspicion': 1535, 'falls': 1536, 'cooper': 1537, 'vanished': 1538, 'slays': 1539, 'buddy': 1540, 'brodie': 1541, 'insure': 1542, 'silence': 1543, 'closes': 1544, 'abetted': 1545, 'superior': 1546, 'paxton': 1547, 'draws': 1548, 'precise': 1549, 'bobs': 1550, 'naturally': 1551, 'prototypical': 1552, 'angry': 1553, 'male': 1554, 'hilt': 1555, 'underplays': 1556, 'characteristic': 1557, 'alert': 1558, 'nonchalance': 1559, 'central': 1560, 'gloria': 1561, 'grahame': 1562, 'fullyfledged': 1563, 'rendition': 1564, 'smartmouthed': 1565, 'vulnerable': 1566, 'tramp': 1567, 'sad': 1568, 'sack': 1569, 'leeched': 1570, 'kelly': 1571, 'haunts': 1572, 'peripheral': 1573, 'memorablebr': 1574, 'politically': 1575, 'engaged': 1576, 'inevitably': 1577, 'succumbs': 1578, 'sermonizing': 1579, 'confined': 1580, 'youngs': 1581, 'reminiscence': 1582, 'grandfather': 1583, 'hands': 1584, 'bigots': 1585, 'thus': 1586, 'chronology': 1587, 'limit': 1588, 'render': 1589, 'explanation': 1590, 'glib': 1591, 'hates': 1592, 'jews': 1593, 'hillbillies': 1594, 'andbr': 1595, 'curiously': 1596, 'survives': 1597, 'wrought': 1598, 'novel': 1599, 'richard': 1600, 'brooks': 1601, 'brick': 1602, 'foxhole': 1603, 'gaybashing': 1604, 'homosexuality': 1605, '1947': 1606, 'pale': 1607, 'holocaust': 1608, 'begun': 1609, 'emerge': 1610, 'ashes': 1611, 'europe': 1612, 'emboldened': 1613, 'register': 1614, 'protest': 1615, 'studios': 1616, 'quaked': 1617, 'prospect': 1618, 'offending': 1619, 'potential': 1620, 'ticket': 1621, 'buyerbr': 1622, 'homophobia': 1623, 'works': 1624, 'general': 1625, 'specifics': 1626, 'smoothly': 1627, 'victims': 1628, 'chatting': 1629, 'lonesome': 1630, 'inviting': 1631, 'girlfriend': 1632, 'tow': 1633, 'raises': 1634, 'question': 1635, 'whether': 1636, 'retained': 1637, 'inadvertently': 1638, 'tipoff': 1639, 'engine': 1640, 'generating': 1641, 'murderous': 1642, 'rage': 1643, 'finest': 1644, 'indian': 1645, 'directed': 1646, 'successful': 1647, 'hitting': 1648, 'parallel': 1649, 'commercial': 1650, 'inspiration': 1651, 'directors': 1652, 'indiabr': 1653, 'reallife': 1654, 'cities': 1655, 'india': 1656, 'encompasses': 1657, 'creates': 1658, 'outlay': 1659, 'environmentbr': 1660, 'amongst': 1661, 'various': 1662, 'officers': 1663, 'colleagues': 1664, 'describes': 1665, 'hotblooded': 1666, 'harsh': 1667, 'constable': 1668, 'himself': 1669, 'suffers': 1670, 'ideologies': 1671, 'incidences': 1672, 'atrocities': 1673, 'immediate': 1674, 'inert': 1675, 'craving': 1676, 'satisfaction': 1677, 'revolved': 1678, 'wherein': 1679, 'efforts': 1680, 'trampled': 1681, 'seniorsthis': 1682, 'leads': 1683, 'achieve': 1684, 'desired': 1685, 'jobsatisfaction': 1686, 'resulting': 1687, 'anger': 1688, 'expressed': 1689, 'excessive': 1690, 'remand': 1691, 'rooms': 1692, 'alcoholicbr': 1693, 'alive': 1694, 'fights': 1695, 'aware': 1696, 'metro': 1697, 'politicians': 1698, 'inertly': 1699, 'associated': 1700, 'compromise': 1701, 'unethical': 1702, 'practice': 1703, 'negative': 1704, 'suspendedbr': 1705, 'master': 1706, 'thoroughly': 1707, 'core': 1708, 'breaks': 1709, 'underworld': 1710, 'gangster': 1711, 'rama': 1712, 'shettys': 1713, 'arrest': 1714, 'short': 1715, 'conversation': 1716, 'hairraising': 1717, 'momentsbr': 1718, 'punch': 1719, 'alcoholism': 1720, 'corruption': 1721, 'influence': 1722, 'courage': 1723, 'deceptions': 1724, 'integral': 1725, 'brilliantlybr': 1726, 'belongs': 1727, 'om': 1728, 'puri': 1729, 'portraying': 1730, 'traversing': 1731, 'emotions': 1732, 'brilliantly': 1733, 'significant': 1734, 'quotes': 1735, 'pronounced': 1736, 'protagonist': 1737, 'mafia': 1738, 'middleman': 1739, 'nondescript': 1740, 'middleaged': 1741, 'salerno': 1742, 'living': 1743, 'elegant': 1744, 'sterile': 1745, 'italianspeaking': 1746, 'canton': 1747, 'switzerland': 1748, 'conducting': 1749, 'business': 1750, 'gradually': 1751, 'pivotal': 1752, 'unremarkable': 1753, 'employees': 1754, 'swiss': 1755, 'normally': 1756, 'count': 1757, 'cash': 1758, '10000': 1759, 'dollars': 1760, 'missing': 1761, 'suitcase': 1762, 'tightly': 1763, 'stacked': 1764, 'banknotes': 1765, 'quietly': 1766, 'icily': 1767, 'threatens': 1768, 'coaxing': 1769, 'manager': 1770, 'close': 1771, 'account': 1772, 'fear': 1773, 'bluffed': 1774, 'told': 1775, 'accepted': 1776, 'initially': 1777, 'scowling': 1778, 'taciturn': 1779, 'curt': 1780, 'verge': 1781, '50': 1782, 'reply': 1783, 'chambermaids': 1784, 'waitresses': 1785, 'hello': 1786, 'goodbye': 1787, 'described': 1788, 'days': 1789, 'nights': 1790, 'oddly': 1791, 'disjoined': 1792, 'deliberate': 1793, 'revealing': 1794, 'seemingly': 1795, 'mundane': 1796, 'details': 1797, 'unnecessary': 1798, 'essential': 1799, 'masterfully': 1800, 'constructed': 1801, 'identity': 1802, 'loving': 1803, 'conveyed': 1804, 'elegantly': 1805, 'boards': 1806, 'canada': 1807, 'stood': 1808, 'treat': 1809, 'mobsters': 1810, 'odds': 1811, 'release': 1812, 'element': 1813, 'protagonists': 1814, 'machinist': 1815, 'explicit': 1816, 'al': 1817, 'pacino': 1818, 'uses': 1819, 'condition': 1820, 'symbolise': 1821, 'deeper': 1822, 'malaise': 1823, 'rammed': 1824, 'deep': 1825, 'obscurity': 1826, 'unconscious': 1827, 'impossible': 1828, 'pinpoint': 1829, 'waitress': 1830, 'sofia': 1831, 'olivia': 1832, 'magnani': 1833, 'granddaughter': 1834, 'legendary': 1835, 'anna': 1836, 'memory': 1837, 'tittas': 1838, 'friend': 1839, 'hasnt': 1840, 'tiny': 1841, 'window': 1842, 'tentatively': 1843, 'accepts': 1844, 'explicitly': 1845, 'spelt': 1846, 'accepting': 1847, 'unimaginable': 1848, 'single': 1849, 'concedes': 1850, 'representative': 1851, 'quiet': 1852, 'taking': 1853, 'italian': 1854, 'cinecittà': 1855, 'waiting': 1856, 'produce': 1857, 'il': 1858, 'postinolike': 1859, 'fare': 1860, 'la': 1861, 'vita': 1862, 'è': 1863, 'bellastyle': 1864, 'neglecting': 1865, 'explore': 1866, 'creations': 1867, 'loss': 1868, 'okay': 1869, 'route': 1870, 'morses': 1871, 'ride': 1872, 'pickle': 1873, 'morse': 1874, 'greatest': 1875, 'coolest': 1876, 'koepp': 1877, 'writer': 1878, 'heavenbr': 1879, 'rubbish': 1880, 'baffles': 1881, 'hope': 1882, 'book': 1883, 'splendored': 1884, 'han': 1885, 'suyin': 1886, 'tackles': 1887, 'issues': 1888, 'race': 1889, 'asians': 1890, 'whites': 1891, 'topic': 1892, 'hans': 1893, 'personal': 1894, 'experiences': 1895, 'eurasian': 1896, 'growing': 1897, 'background': 1898, 'daring': 1899, 'remembered': 1900, 'jones': 1901, 'oscar': 1902, 'doctor': 1903, 'mixed': 1904, 'breed': 1905, 'advent': 1906, 'communism': 1907, 'mainland': 1908, 'william': 1909, 'holden': 1910, 'journalist': 1911, 'covering': 1912, 'regions': 1913, 'notch': 1914, 'chemistry': 1915, 'provides': 1916, 'affection': 1917, 'melt': 1918, 'hearts': 1919, 'romantically': 1920, 'inclinedbr': 1921, 'fiftys': 1922, 'hilltop': 1923, 'overlooking': 1924, 'harbor': 1925, 'intimate': 1926, 'ending': 1927, 'tearjerker': 1928, 'consider': 1929, 'sentimental': 1930, 'romances': 1931, 'passé': 1932, 'stories': 1933, 'shining': 1934, 'example': 1935, 'basically': 1936, 'boy': 1937, 'thinks': 1938, 'zombie': 1939, 'slower': 1940, 'soap': 1941, 'opera': 1942, 'suddenly': 1943, 'rambo': 1944, 'zombiebr': 1945, 'ok': 1946, 'watchable': 1947, 'divorcing': 1948, 'arguing': 1949, 'ruins': 1950, 'expected': 1951, 'boogeyman': 1952, 'meaningless': 1953, 'spotsbr': 1954, '3': 1955, 'descent': 1956, 'dialogs': 1957, 'ignore': 1958, 'amazing': 1959, 'fresh': 1960, 'aired': 1961, '7': 1962, '8': 1963, 'dropped': 1964, '1990': 1965, 'anymore': 1966, 'continued': 1967, 'further': 1968, 'complete': 1969, 'todaybr': 1970, 'disgraceful': 1971, 'fallen': 1972, 'painfully': 1973, 'mildly': 1974, 'respite': 1975, 'guesthosts': 1976, 'creator': 1977, 'handselected': 1978, 'chose': 1979, 'hacks': 1980, 'recognize': 1981, 'brilliance': 1982, 'replace': 1983, 'mediocrity': 1984, 'stars': 1985, 'respect': 1986, 'huge': 1987, 'encouraged': 1988, 'positive': 1989, '950': 1990, 'pacing': 1991, 'country': 1992, 'tune': 1993, 'four': 1994, 'extreme': 1995, 'rarely': 1996, 'credits': 1997, 'prevents': 1998, '1score': 1999, 'harvey': 2000, 'obsessives': 2001, 'phil': 2002, 'alien': 2003, 'quirky': 2004, 'humour': 2005, 'actual': 2006, 'punchlinesbr': 2007, 'progressed': 2008, 'anymorebr': 2009, 'low': 2010, 'budget': 2011, 'interestbr': 2012, 'imagine': 2013, 'stoner': 2014, 'currently': 2015, 'partakingbr': 2016, 'planet': 2017, 'scariest': 2018, 'bird': 2019, 'dangling': 2020, 'helplessly': 2021, 'parachutes': 2022, 'horror': 2023, 'horrorbr': 2024, 'cheesy': 2025, 'saturday': 2026, 'afternoons': 2027, 'tired': 2028, 'type': 2029, 'woman': 2030, 'might': 2031, 'professor': 2032, 'resolution': 2033, 'care': 2034, 'angle': 2035, 'plots': 2036, 'unintentional': 2037, 'humorbr': 2038, 'later': 2039, 'psycho': 2040, 'loved': 2041, 'janet': 2042, 'leigh': 2043, 'bumped': 2044, 'early': 2045, 'sat': 2046, 'notice': 2047, 'screenwriters': 2048, 'scary': 2049, 'possible': 2050, 'wellworn': 2051, 'rules': 2052, 'postal': 2053, 'rights': 2054, 'ago': 2055, 'game': 2056, 'finsished': 2057, 'killing': 2058, 'mercs': 2059, 'infiltrating': 2060, 'secret': 2061, 'labs': 2062, 'located': 2063, 'tropical': 2064, 'schemed': 2065, 'legion': 2066, 'schmucks': 2067, 'loneley': 2068, 'invites': 2069, 'countrymen': 2070, 'players': 2071, 'ralf': 2072, 'moellerbr': 2073, 'selfs': 2074, 'biz': 2075, 'tale': 2076, 'jack': 2077, 'german': 2078, 'hail': 2079, 'bratwurst': 2080, 'dudes': 2081, 'tils': 2082, 'badass': 2083, 'complained': 2084, 'staying': 2085, 'perspective': 2086, 'kicking': 2087, 'demented': 2088, 'evil': 2089, 'mad': 2090, 'scientist': 2091, 'dr': 2092, 'krieger': 2093, 'geneticallymutatedsoldiers': 2094, 'performing': 2095, 'topsecret': 2096, 'spoiler': 2097, 'vancouver': 2098, 'palm': 2099, 'trees': 2100, 'rich': 2101, 'lumberjackwoods': 2102, 'gone': 2103, 'started': 2104, 'mehehe': 2105, 'stay': 2106, 'shenanigans': 2107, 'meaning': 2108, 'suckbr': 2109, 'mentioning': 2110, 'imply': 2111, 'areas': 2112, 'boat': 2113, 'cromedalbino': 2114, 'squad': 2115, 'enters': 2116, 'laugh': 2117, 'reeks': 2118, 'scheisse': 2119, 'poop': 2120, 'simpletons': 2121, 'wiff': 2122, 'ahead': 2123, 'btw': 2124, 'sidekick': 2125, 'shakespearebr': 2126, 'lostbr': 2127, 'masses': 2128, 'ruin': 2129, 'goodbr': 2130, 'certain': 2131, 'rev': 2132, 'bowdler': 2133, 'hence': 2134, 'bowdlerization': 2135, 'victorian': 2136, 'erabr': 2137, 'words': 2138, 'improve': 2139, 'perfectionbr': 2140, 'text': 2141, 'composition': 2142, 'forte': 2143, 'saying': 2144, 'cut': 2145, 'drawn': 2146, 'erotic': 2147, 'amateurish': 2148, 'bits': 2149, 'project': 2150, 'rosanna': 2151, 'arquette': 2152, 'thinking': 2153, 'stock': 2154, 'midwest': 2155, 'involved': 2156, 'lessons': 2157, 'learned': 2158, 'insights': 2159, 'stilted': 2160, 'skin': 2161, 'intrigues': 2162, 'videotaped': 2163, 'nonsensewhat': 2164, 'bisexual': 2165, 'nowhere': 2166, 'heterosexual': 2167, 'encounters': 2168, 'dance': 2169, 'stereotyped': 2170, 'pass': 2171, 'million': 2172, 'wasted': 2173, 'spent': 2174, 'starving': 2175, 'aids': 2176, 'africa': 2177, 'continuous': 2178, 'minute': 2179, 'busy': 2180, 'running': 2181, 'sword': 2182, 'attachment': 2183, 'wanted': 2184, 'destroy': 2185, 'blatantly': 2186, 'lotr': 2187, 'matrix': 2188, 'examplesbr': 2189, 'ghost': 2190, 'yoda': 2191, 'obee': 2192, 'vader': 2193, 'frodo': 2194, 'attacked': 2195, 'return': 2196, 'elijah': 2197, 'wood': 2198, 'waitit': 2199, 'hypnotizes': 2200, 'stings': 2201, 'wraps': 2202, 'upuh': 2203, 'hellobr': 2204, 'vs': 2205, 'humans': 2206, 'matrixor': 2207, 'terminatorbr': 2208, 'examples': 2209, 'line': 2210, 'rushed': 2211, 'conclusion': 2212, 'childrens': 2213, 'adult': 2214, 'either': 2215, 'disappointment': 2216, 'stinkers': 2217, 'golden': 2218, 'globe': 2219, 'female': 2220, 'painter': 2221, 'mangled': 2222, 'complaint': 2223, 'liberties': 2224, 'perfectly': 2225, 'accounts': 2226, 'artist': 2227, 'dishwaterdull': 2228, 'suppose': 2229, 'naked': 2230, 'factual': 2231, 'hurriedly': 2232, 'capped': 2233, 'summary': 2234, 'artists': 2235, 'saved': 2236, 'ourselves': 2237, 'couple': 2238, 'favored': 2239, 'brevity': 2240, 'misfortune': 2241, 'entiretybr': 2242, 'shouldnt': 2243, 'fmovie': 2244, 'paperthin': 2245, 'fake': 2246, 'funnyalmostbr': 2247, 'packed': 2248, 'oneliners': 2249, 'respectable': 2250, 'amusing': 2251, 'bitbr': 2252, 'geared': 2253, 'women': 2254, 'unattractive': 2255, 'wrinkled': 2256, 'appear': 2257, 'fail': 2258, 'miserablybr': 2259, 'laughs': 2260, 'straight': 2261, 'used': 2262, 'preschool': 2263, 'theyre': 2264, 'c': 2265, 'lotbr': 2266, 'moving': 2267, 'sudden': 2268, 'boom': 2269, 'wwwaaaaayyyyy': 2270, 'downhillbr': 2271, 'crissakes': 2272, 'vividly': 2273, 'bunch': 2274, 'dinosaurs': 2275, 'addition': 2276, 'transition': 2277, 'unorganized': 2278, 'voicesespecially': 2279, 'dub': 2280, 'viewed': 2281, 'horrid': 2282, 'begging': 2283, 'tape': 2284, 'vhs': 2285, 'player': 2286, 'kept': 2287, 'surviving': 2288, 'cracking': 2289, 'robots': 2290, 'joelmike': 2291, 'mst3k': 2292, 'pick': 2293, 'survive': 2294, 'heck': 2295, 'planning': 2296, 'fellow': 2297, 'otaku': 2298, 'pal': 2299, 'mine': 2300, 'halloween': 2301, 'night': 2302, 'stupid': 2303, 'improvement': 2304, '0510': 2305, 'according': 2306, 'grading': 2307, 'scale': 2308, 'means': 2309, 'worldfest': 2310, 'received': 2311, 'applause': 2312, 'afterwards': 2313, 'receiving': 2314, 'known': 2315, 'jbeals': 2316, 'mparker': 2317, 'allowed': 2318, 'judge': 2319, 'therefore': 2320, 'bore': 2321, 'depth': 2322, 'revolving': 2323, 'feels': 2324, 'straighttovideo': 2325, 'standardsbr': 2326, 'stinging': 2327, 'satire': 2328, 'sappy': 2329, 'values': 2330, 'promo': 2331, 'list': 2332, 'miserably': 2333, 'endbr': 2334, 'inc': 2335, 'depresses': 2336, 'clumsy': 2337, 'targets': 2338, 'reflect': 2339, 'serious': 2340, 'particular': 2341, 'corporatization': 2342, 'poking': 2343, 'diminishes': 2344, 'atrocity': 2345, 'similarly': 2346, 'trivializes': 2347, 'frustrating': 2348, 'energetic': 2349, 'prepared': 2350, 'able': 2351, 'looping': 2352, 'americas': 2353, 'funniest': 2354, 'videos': 2355, 'damn': 2356, 'latino': 2357, 'speak': 2358, 'responsible': 2359, 'transcends': 2360, 'gloriously': 2361, 'badness': 2362, 'dancing': 2363, 'exposure': 2364, 'templarios': 2365, 'excited': 2366, 'among': 2367, 'offerings': 2368, 'anchor': 2369, 'bay': 2370, 'cult': 2371, 'classics': 2372, 'baby': 2373, 'print': 2374, 'quality': 2375, 'alone': 2376, 'hide': 2377, 'deadly': 2378, 'dull': 2379, 'thrilling': 2380, 'opening': 2381, 'villagers': 2382, 'exact': 2383, 'revenge': 2384, 'templars': 2385, 'motion': 2386, 'ponderous': 2387, 'unfulfilling': 2388, 'adding': 2389, 'insult': 2390, 'injury': 2391, 'dubbed': 2392, 'subtitled': 2393, 'promised': 2394, 'jacket': 2395, 'expecting': 2396, 'pack': 2397, '5': 2398, 'fiver': 2399, 'expect': 2400, 'occasional': 2401, 'camcorder': 2402, 'ie': 2403, 'damned': 2404, 'assume': 2405, 'build': 2406, 'tension': 2407, 'thumb': 2408, 'fast': 2409, 'button': 2410, 'press': 2411, 'gave': 2412, 'seriously': 2413, 'coz': 2414, 'meercat': 2415, 'gonna': 2416, 'explaining': 2417, 'anyway': 2418, 'concerned': 2419, 'talent': 2420, 'avoid': 2421, 'bored': 2422, 'paint': 2423, 'dry': 2424, '300': 2425, 'sounded': 2426, 'ranmaesque': 2427, 'dragging': 2428, 'skeleton': 2429, 'cute': 2430, 'viewing': 2431, 'sweet': 2432, 'indie': 2433, 'edge': 2434, '100': 2435, 'wrongbr': 2436, 'wonder': 2437, 'hardly': 2438, 'foul': 2439, 'language': 2440, 'closest': 2441, 'nudity': 2442, 'hoping': 2443, 'nightgown': 2444, 'antireligious': 2445, 'humor': 2446, 'tame': 2447, 'caricatured': 2448, 'insincere': 2449, 'derivative': 2450, 'unoriginal': 2451, 'slightestit': 2452, 'listen': 2453, 'stevens': 2454, 'jesus': 2455, 'wear': 2456, 'rolex': 2457, 'television': 2458, 'qualify': 2459, 'refuses': 2460, '17': 2461, 'thisas': 2462, 'pornographic': 2463, 'requiem': 2464, 'insist': 2465, 'zack': 2466, 'reba': 2467, 'worsebr': 2468, 'waybr': 2469, 'worries': 2470, 'methe': 2471, 'offend': 2472, 'needs': 2473, 'portrayed': 2474, 'ones': 2475, 'virgin': 2476, 'r': 2477, 'purely': 2478, 'aspect': 2479, 'eleven': 2480, 'twelve': 2481, 'causes': 2482, 'number': 2483, 'chances': 2484, 'teens': 2485, '210': 2486, 'investigate': 2487, 'dunnit': 2488, 'including': 2489, 'withbr': 2490, 'achingly': 2491, 'heroine': 2492, 'menace': 2493, 'foreboding': 2494, 'thunderstorms': 2495, 'strangely': 2496, 'housegreat': 2497, 'double': 2498, 'glazing': 2499, 'serves': 2500, 'purpose': 2501, 'quick': 2502, 'gory': 2503, 'tedium': 2504, 'unbearable': 2505, 'suggests': 2506, 'spate': 2507, 'throughout': 2508, 'area': 2509, 'apparent': 2510, 'ritual': 2511, 'salt': 2512, 'pepper': 2513, 'sums': 2514, 'inherent': 2515, 'directionbr': 2516, 'add': 2517, 'act': 2518, 'willing': 2519, 'completely': 2520, 'nude': 2521, 'shower': 2522, 'hopebr': 2523, 'following': 2524, 'banned': 2525, 'uk': 2526, '80s': 2527, 'extended': 2528, 'curiosity': 2529, 'value': 2530, 'daft': 2531, 'worryits': 2532, 'telegraphed': 2533, 'beforebr': 2534, 'woods': 2535, 'steep': 2536, 'upward': 2537, 'slope': 2538, 'obviously': 2539, 'figure': 2540, 'dressed': 2541, 'brandishing': 2542, 'large': 2543, 'scythe': 2544, 'slide': 2545, 'conveniently': 2546, 'upright': 2547, 'front': 2548, 'weaponbr': 2549}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_reviews=list()\n",
        "for review in all_reviews:\n",
        "  encoded_review=list()\n",
        "  for word in review.split():\n",
        "    if word not in vocab_to_int.keys():\n",
        "      #if word is not available in vocab_to_int put 0 in that place\n",
        "      encoded_review.append(0)\n",
        "    else:\n",
        "      encoded_review.append(vocab_to_int[word])\n",
        "  encoded_reviews.append(encoded_review)"
      ],
      "metadata": {
        "id": "Uz3jgermSMXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "'''\n",
        "this step will Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
        "'''\n",
        "sequence_length=250\n",
        "features=np.zeros((len(encoded_reviews), sequence_length), dtype=int)\n",
        "for i, review in enumerate(encoded_reviews):\n",
        "  review_len=len(review)\n",
        "  if (review_len<=sequence_length):\n",
        "    zeros=list(np.zeros(sequence_length-review_len))\n",
        "    new=zeros+review\n",
        "  else:\n",
        "    new=review[:sequence_length]\n",
        "features[i,:]=np.array(new)"
      ],
      "metadata": {
        "id": "InxnEGFDSQG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Our dataset has ‘positive’ and ‘negative’ as a label, it will be easy if we have 1 and 0, instead of ‘positive’ and ‘negative’\n",
        "labels=[1 if label.strip()=='positive' else 0 for label in imdb_dataset.sentiment.to_list()]"
      ],
      "metadata": {
        "id": "l0lm28xjSY3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train, validation, and test set splits"
      ],
      "metadata": {
        "id": "63xHJ7aco8__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split_dataset into 80% training , 10% test and 10% Validation Dataset\n",
        "train_x=features[:int(0.6*len(features))]\n",
        "train_y=labels[:int(0.6*len(features))]\n",
        "valid_x=features[int(0.6*len(features)):int(0.8*len(features))]\n",
        "valid_y=labels[int(0.6*len(features)):int(0.8*len(features))]\n",
        "test_x=features[int(0.8*len(features)):]\n",
        "test_y=labels[int(0.8*len(features)):]\n",
        "print(len(train_y), len(valid_y), len(test_y))"
      ],
      "metadata": {
        "id": "xoOnFglvSY6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf6c782-8db2-43eb-d5ef-d94ae9cd9a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24 8 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "#create Tensor Dataset\n",
        "train_data=TensorDataset(torch.LongTensor(train_x), torch.FloatTensor(train_y)) #Changed to LongTensor\n",
        "valid_data=TensorDataset(torch.LongTensor(valid_x), torch.FloatTensor(valid_y)) #Changed to LongTensor\n",
        "test_data=TensorDataset(torch.LongTensor(test_x), torch.FloatTensor(test_y)) #Changed to LongTensor\n",
        "\n",
        "#dataloader\n",
        "batch_size=24\n",
        "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "valid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "qh0PVeWAS4H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM model specification\n"
      ],
      "metadata": {
        "id": "kVO0MbbapF2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic implementation of an LSTM for binary sentiment classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.output_size=output_size\n",
        "        self.n_layers=n_layers\n",
        "        self.hidden_dim=hidden_dim\n",
        "\n",
        "        #Embedding and LSTM layers\n",
        "        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        #dropout layer\n",
        "        self.dropout=nn.Dropout(0.3)\n",
        "\n",
        "        #Linear and sigmoid layer\n",
        "        self.fc1=nn.Linear(hidden_dim, 64)\n",
        "        self.fc2=nn.Linear(64, 16)\n",
        "        self.fc3=nn.Linear(16,output_size)\n",
        "        self.sigmoid=nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size=x.size()\n",
        "\n",
        "        #Embadding and LSTM output\n",
        "        embedd=self.embedding(x)\n",
        "        lstm_out, hidden=self.lstm(embedd, hidden)\n",
        "\n",
        "        #stack up the lstm output\n",
        "        lstm_out=lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        #dropout and fully connected layers\n",
        "        out=self.dropout(lstm_out)\n",
        "        out=self.fc1(out)\n",
        "        out=self.dropout(out)\n",
        "        out=self.fc2(out)\n",
        "        out=self.dropout(out)\n",
        "        out=self.fc3(out)\n",
        "        sig_out=self.sigmoid(out)\n",
        "\n",
        "        sig_out=sig_out.view(batch_size, -1)\n",
        "        sig_out=sig_out[:, -1]\n",
        "\n",
        "        return sig_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initialize Hidden STATE\"\"\"\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "E1HsdxfITs5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate the model with hyperparameters"
      ],
      "metadata": {
        "id": "peGTlJnApMR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "drop_prob = 0.5\n",
        "\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob)\n",
        "print(net)\n"
      ],
      "metadata": {
        "id": "kY3SwqXyT1Sj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f307fbc8-75d4-407e-892f-8e72a838ae17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(2550, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=16, bias=True)\n",
            "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "RmOM-7OroEKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = False#torch.cuda.is_available()\n",
        "\n",
        "# training params\n",
        "epochs = 1\n",
        "\n",
        "counter = 0\n",
        "print_every = 10\n",
        "clip = 5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    print(f\"epoch {e}\")\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        if(train_on_gpu):\n",
        "            inputs=inputs.cuda()\n",
        "            labels=labels.cuda()\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero out accumulated gradients\n",
        "        #net.zero_grad()\n",
        "        #get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float()) # Changed labels.long() to labels.float()\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                #inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "metadata": {
        "id": "z-oraRKNZ0Ze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be7a6680-7ba1-4f09-9f9d-5d04dea0d6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n"
      ],
      "metadata": {
        "id": "epXKuwi2oMK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(8)\n",
        "blah = True\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "  # Creating new variables for the hidden state, otherwise\n",
        "  # we'd backprop through the entire training history\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  #inputs, labels = inputs.cuda(), labels.cuda()\n",
        "  output, h = net(inputs, h)\n",
        "  # calculate loss\n",
        "  test_loss = criterion(output.squeeze(), labels.float())\n",
        "  test_losses.append(test_loss.item())\n",
        "\n",
        "  # convert output probabilities to predicted class (0 or 1)\n",
        "  pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "\n",
        "  # compare predictions to true label\n",
        "  correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "  num_correct += np.sum(correct)\n",
        "\n",
        "  # avg test loss\n",
        "  print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "  # accuracy over all test data\n",
        "  test_acc = num_correct/len(test_loader.dataset)\n",
        "  print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "9FfPCroVbaJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10ac843-d5e5-47f9-e631-22eee259c1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.855\n",
            "Test accuracy: 0.000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}