{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBw__kO4P8-h"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KbGaKcUP8-j"
      },
      "source": [
        "LSTMs in Pytorch\n",
        "----------------\n",
        "\n",
        "Before getting to the example, note a few things. Pytorch\\'s LSTM\n",
        "expects all of its inputs to be 3D tensors. The semantics of the axes of\n",
        "these tensors is important. The first axis is the sequence itself, the\n",
        "second indexes instances in the mini-batch, and the third indexes\n",
        "elements of the input. We haven\\'t discussed mini-batching, so let\\'s\n",
        "just ignore that and assume we will always have just 1 dimension on the\n",
        "second axis. If we want to run the sequence model over the sentence\n",
        "\\\"The cow jumped\\\", our input should look like\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\begin{bmatrix}\n",
        "\\overbrace{q_\\text{The}}^\\text{row vector} \\\\\n",
        "q_\\text{cow} \\\\\n",
        "q_\\text{jumped}\n",
        "\\end{bmatrix}\n",
        "\\end{aligned}$$\n",
        "\n",
        "Except remember there is an additional 2nd dimension with size 1.\n",
        "\n",
        "In addition, you could go through the sequence one at a time, in which\n",
        "case the 1st axis will have size 1 also.\n",
        "\n",
        "Let\\'s see a quick example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c1NjwFwP8-k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmildQ69P8-l"
      },
      "outputs": [],
      "source": [
        "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
        "\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3))\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "\n",
        "# alternatively, we can do the entire sequence all at once.\n",
        "# the first value returned by LSTM is all of the hidden states throughout\n",
        "# the sequence. the second is just the most recent hidden state\n",
        "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
        "# The reason for this is that:\n",
        "# \"out\" will give you access to all hidden states in the sequence\n",
        "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
        "# by passing it as an argument  to the lstm at a later time\n",
        "# Add the extra 2nd dimension\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "print(hidden)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "imdb_dataset = pd.read_csv(\"imdb_small.csv\")\n",
        "#take a look at the data\n",
        "imdb_dataset.head()"
      ],
      "metadata": {
        "id": "_AUa14R0RDXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        return text, label"
      ],
      "metadata": {
        "id": "YlQlhDKbjKix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        " Remove Punctuation and get all the words from review dataset. Count all the words and sort it based on counts\n",
        "\n"
      ],
      "metadata": {
        "id": "Tlv9lXOvRbmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"This is a positive sentence.\", \"This is a negative sentence.\"]\n",
        "labels = [1, 0]\n",
        "\n",
        "dataset = TextDataset(texts, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=2)\n",
        "\n",
        "for batch in dataloader:\n",
        "    texts_batch, labels_batch = batch\n",
        "    print(texts_batch)\n",
        "    print(labels_batch)"
      ],
      "metadata": {
        "id": "pei6OajTjQTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from collections import Counter\n",
        "all_reviews=list()\n",
        "for text in imdb_dataset.review.to_list():\n",
        "  text = text.lower()\n",
        "  text = \"\".join([ch for ch in text if ch not in punctuation])\n",
        "  all_reviews.append(text)\n",
        "all_text = \" \".join(all_reviews)\n",
        "all_words = all_text.split()\n",
        "\n",
        "# Count all the words using Counter Method\n",
        "count_words = Counter(all_words)\n",
        "total_words=len(all_words)\n",
        "sorted_words=count_words.most_common(total_words)\n",
        "print(f\"Top ten occuring words : {sorted_words[:10]}\")\n"
      ],
      "metadata": {
        "id": "1Ev-QYIORByG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        " Create a dictionary to convert words to Integers based on the number of occurrence of the word"
      ],
      "metadata": {
        "id": "2PM2wDxESAwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "we will start creating dictionary with index 1 because 0 is reserved for padding\n",
        "'''\n",
        "\n",
        "vocab_to_int={w:i+1 for i,(w,c) in enumerate(sorted_words)}\n",
        "print(vocab_to_int)"
      ],
      "metadata": {
        "id": "yMcFeNbtR_9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_reviews=list()\n",
        "for review in all_reviews:\n",
        "  encoded_review=list()\n",
        "  for word in review.split():\n",
        "    if word not in vocab_to_int.keys():\n",
        "      #if word is not available in vocab_to_int put 0 in that place\n",
        "      encoded_review.append(0)\n",
        "    else:\n",
        "      encoded_review.append(vocab_to_int[word])\n",
        "  encoded_reviews.append(encoded_review)"
      ],
      "metadata": {
        "id": "Uz3jgermSMXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "'''\n",
        "this step will Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
        "'''\n",
        "sequence_length=250\n",
        "features=np.zeros((len(encoded_reviews), sequence_length), dtype=int)\n",
        "for i, review in enumerate(encoded_reviews):\n",
        "  review_len=len(review)\n",
        "  if (review_len<=sequence_length):\n",
        "    zeros=list(np.zeros(sequence_length-review_len))\n",
        "    new=zeros+review\n",
        "  else:\n",
        "    new=review[:sequence_length]\n",
        "features[i,:]=np.array(new)"
      ],
      "metadata": {
        "id": "InxnEGFDSQG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Our dataset has ‘positive’ and ‘negative’ as a label, it will be easy if we have 1 and 0, instead of ‘positive’ and ‘negative’\n",
        "labels=[1 if label.strip()=='positive' else 0 for label in imdb_dataset.sentiment.to_list()]"
      ],
      "metadata": {
        "id": "l0lm28xjSY3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train, validation, and test set splits"
      ],
      "metadata": {
        "id": "63xHJ7aco8__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split_dataset into 80% training , 10% test and 10% Validation Dataset\n",
        "train_x=features[:int(0.8*len(features))]\n",
        "train_y=labels[:int(0.8*len(features))]\n",
        "valid_x=features[int(0.8*len(features)):int(0.9*len(features))]\n",
        "valid_y=labels[int(0.8*len(features)):int(0.9*len(features))]\n",
        "test_x=features[int(0.9*len(features)):]\n",
        "test_y=labels[int(0.9*len(features)):]\n",
        "print(len(train_y), len(valid_y), len(test_y))"
      ],
      "metadata": {
        "id": "xoOnFglvSY6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "#create Tensor Dataset\n",
        "train_data=TensorDataset(torch.LongTensor(train_x), torch.FloatTensor(train_y)) #Changed to LongTensor\n",
        "valid_data=TensorDataset(torch.LongTensor(valid_x), torch.FloatTensor(valid_y)) #Changed to LongTensor\n",
        "test_data=TensorDataset(torch.LongTensor(test_x), torch.FloatTensor(test_y)) #Changed to LongTensor\n",
        "\n",
        "#dataloader\n",
        "batch_size=50\n",
        "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "valid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "qh0PVeWAS4H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM model specification\n"
      ],
      "metadata": {
        "id": "kVO0MbbapF2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic implementation of an LSTM for binary sentiment classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.output_size=output_size\n",
        "        self.n_layers=n_layers\n",
        "        self.hidden_dim=hidden_dim\n",
        "\n",
        "        #Embedding and LSTM layers\n",
        "        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        #dropout layer\n",
        "        self.dropout=nn.Dropout(0.3)\n",
        "\n",
        "        #Linear and sigmoid layer\n",
        "        self.fc1=nn.Linear(hidden_dim, 64)\n",
        "        self.fc2=nn.Linear(64, 16)\n",
        "        self.fc3=nn.Linear(16,output_size)\n",
        "        self.sigmoid=nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size=x.size()\n",
        "\n",
        "        #Embadding and LSTM output\n",
        "        embedd=self.embedding(x)\n",
        "        lstm_out, hidden=self.lstm(embedd, hidden)\n",
        "\n",
        "        #stack up the lstm output\n",
        "        lstm_out=lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        #dropout and fully connected layers\n",
        "        out=self.dropout(lstm_out)\n",
        "        out=self.fc1(out)\n",
        "        out=self.dropout(out)\n",
        "        out=self.fc2(out)\n",
        "        out=self.dropout(out)\n",
        "        out=self.fc3(out)\n",
        "        sig_out=self.sigmoid(out)\n",
        "\n",
        "        sig_out=sig_out.view(batch_size, -1)\n",
        "        sig_out=sig_out[:, -1]\n",
        "\n",
        "        return sig_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initialize Hidden STATE\"\"\"\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "E1HsdxfITs5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate the model with hyperparameters"
      ],
      "metadata": {
        "id": "peGTlJnApMR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "dropout_prob = 0.5\n",
        "\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob)\n",
        "print(net)\n"
      ],
      "metadata": {
        "id": "kY3SwqXyT1Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "RmOM-7OroEKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "lr = 0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "# training params\n",
        "epochs = 4\n",
        "\n",
        "counter = 0\n",
        "print_every = 10\n",
        "clip = 5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    print(f\"epoch {e}\")\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        if(train_on_gpu):\n",
        "            inputs=inputs.cuda()\n",
        "            labels=labels.cuda()\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero out accumulated gradients\n",
        "        #net.zero_grad()\n",
        "        #get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float()) # Changed labels.long() to labels.float()\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "metadata": {
        "id": "z-oraRKNZ0Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n"
      ],
      "metadata": {
        "id": "epXKuwi2oMK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "  # Creating new variables for the hidden state, otherwise\n",
        "  # we'd backprop through the entire training history\n",
        "  h = tuple([each.data for each in h])\n",
        "  if(train_on_gpu):\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    output, h = net(inputs, h)\n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "    # avg test loss\n",
        "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "    # accuracy over all test data\n",
        "    test_acc = num_correct/len(test_loader.dataset)\n",
        "    print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "9FfPCroVbaJC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}