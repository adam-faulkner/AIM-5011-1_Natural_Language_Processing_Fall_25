{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VY76ydh7ksw"
      },
      "source": [
        "### Assignment 4 section 4: Create a recipe generator using RAG\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "NOTE: This code should be run with a HuggingFace account with a collab CPU runtime and the option `High Ram`clicked.\n",
        "\n",
        "```\n",
        "\n",
        "For this assignment, you'll be creating a RAG-based recipe genertion system using Alibaba's `KingNish/Qwen2.5-0.5b-Test-ft` LLM, a small but high-accuracy LLM that can be run on a CPU. To implement the RAG component, we'll be using the `LlamaIndex` library and, as our embedding model, `BAAI/bge-small-en-v1.5`\n",
        "\n",
        "This code should be run with a HuggingFace account with a collab CPU runtime and the option `High Ram`clicked.\n",
        "\n",
        "The recipe dataset we'll be using is `m3hrdadfi/recipe_nlg_lite`. The train split of the dataset will be used for the index and a portion of the test dataset will be used to qualitiatively evaluate the RAG system. You'll also be asked to compare these results qualitatively with vanilla `Qwen2.5-0.5b-Test-ft`, i.e., prompt the `Qwen` base LLM for recipes and compare these to the RAG system to determine if there are any benefits to using RAG for this task.\n",
        "\n",
        "Your task is to\n",
        "\n",
        "1. complete the code in each of the cells below by inserting your own code wherevver you see  `### WRITE YOUR CODE HERE ###` Also, add a comment wherever you see `### WRITE YOUR COMMENT HERE ###`  You'll likely want to consult the LlamaIndex documenetation to complete much of this code. The LlamaIndex docs on RAG can be found [here](https://docs.llamaindex.ai/en/stable/understanding/rag/)\n",
        "\n",
        "2. run all of cells after completing the code and answer the questions at the end of the notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LADY6MiB7ksy"
      },
      "outputs": [],
      "source": [
        "# uncomment and run in your environment / on Colab, if you haven't installed these packages yet\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install sentence-transformers\n",
        "!pip install datasets\n",
        "!pip install llama-index\n",
        "!pip install \"transformers[torch]\" \"huggingface_hub[inference]\"\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyKwz8r57ks0"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "from llama_index.core import VectorStoreIndex, Settings, Document\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMMJL-6f7ks0"
      },
      "outputs": [],
      "source": [
        "# load dataset from HF\n",
        "dataset = load_dataset(\"m3hrdadfi/recipe_nlg_lite\")\n",
        "# convert train split to pandas dataframe\n",
        "dataset_df = pd.DataFrame(dataset[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK6JT2It7ks0"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the data\n",
        "dataset_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E85uQd9w7ks1"
      },
      "outputs": [],
      "source": [
        "# We'll crate a VectorStorageIndex with the texts from the train dataset. These will be formatted as\n",
        "#\"Name of recipe \\n\\n ingredients \\n\\n steps\"\n",
        "\n",
        "texts = [\n",
        "    f\"{row['name']} \\n\\n {row['ingredients']} \\n\\n {row['steps']}\" for _, row in dataset_df.iterrows()\n",
        "]\n",
        "texts[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMDVXnHV7ks1"
      },
      "outputs": [],
      "source": [
        "#We then load the texts into LlamaIndex's Document object. Later we'll load these into a vector database\n",
        "documents = [Document(text=t) for t in texts]\n",
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a utility function to format prompts. This will only need to be edited if you use something other than Qwen. If you\n",
        "# decide to use something other than Qwen, check the HuggingFace model card for that model to determine prompt formatting\n",
        "def completion_to_prompt(completion):\n",
        "    return f\"{completion}\""
      ],
      "metadata": {
        "id": "iWwe5yBJHKt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv-UR8GC9Ypa"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate # Using `bitsandbytes` 8-bit quantization requires Accelerate\n",
        "!pip install -U bitsandbytes\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooXpfDit7ks1"
      },
      "outputs": [],
      "source": [
        "# Save the setting reused by our RAG system across queries and specify the embedding model--\n",
        "# the model we'll be using is `BAAI/bge-small-en-v1.5` but you're welcome to use another\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "      ### WRITE YOUR CODE HERE ###\n",
        ")\n",
        "\n",
        "# pass the LLM to our the settings object\n",
        "# see these docs for more details: https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/\n",
        "# and https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom/\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "     ### WRITE YOUR CODE HERE ###\n",
        "    model_name= ,\n",
        "    ### WRITE YOUR CODE HERE ###\n",
        "    tokenizer_name=  ,\n",
        "    context_window=1024,\n",
        "    max_new_tokens=128,\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True},\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    device_map=\"auto\",\n",
        "    # Explicitly set load_in_8bit=False or remove it -- up to you\n",
        "    model_kwargs={\"torch_dtype\": torch.float16, \"trust_remote_code\": True}, # \"load_in_8bit\": True\n",
        ")\n",
        "print(\"Set the LLM as KingNish/Qwen2.5-0.5b-Test-ft...\")\n",
        "\n",
        "# Now we create a vector store which converts the documents to Node objects as per\n",
        "# (https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/)\n",
        "print(\"Creating index...\")\n",
        "index = VectorStoreIndex.from_documents(\n",
        "   ### WRITE YOUR CODE HERE ###\n",
        "                              # note: this may take a while on a CPU so go do something else in the meanwhile\n",
        ")\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVqHa9E67ks2"
      },
      "outputs": [],
      "source": [
        "# https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/\n",
        "# we define the query engine: generic interface that allows to ask questions over data\n",
        "query_engine = index.as_query_engine(\n",
        "    ### WRITE YOUR COMMENT HERE: What does compact do?###\n",
        "    response_mode=\"compact\",\n",
        "    ### WRITE YOUR COMMENT HERE: What does similarity_top_k specify?###\n",
        "    similarity_top_k=3,\n",
        "    verbose=True,\n",
        ")\n",
        "# https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/\n",
        "response = query_engine.query(\"How do I make creme brulee?\")\n",
        "print(response)\n",
        "\n",
        "for i, n in enumerate(response.source_nodes):\n",
        "    print(f\"----- Node {i} -----\")\n",
        "    print(n.node.get_content())\n",
        "    print(\"score\")\n",
        "    print(n.score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8ymksiZ7ks2"
      },
      "outputs": [],
      "source": [
        "# testing loop\n",
        "rag_responses = []\n",
        "vanilla_responses = []\n",
        "retrieved_node_texts = []\n",
        "retrieved_node_scores = []\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"KingNish/Qwen2.5-0.5b-Test-ft\",\n",
        "    #device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"KingNish/Qwen2.5-0.5b-Test-ft\")\n",
        "\n",
        "# retrieve 20 random dish names from test dataset to test the system on\n",
        "test_df = pd.DataFrame(dataset[\"test\"]).sample(20)\n",
        "test_queries = [f'How do I make creme brulee?']\n",
        "    #f'How do I make {r[\"name\"]}?' for\n",
        "    #_, r in test_df.iterrows()\n",
        "#]\n",
        "#print(test_queries[:5])\n",
        "\n",
        "for query in test_queries:# you may want to just run a few of these via [:5]:\n",
        "    ### WRITE YOUR CODE HERE ###\n",
        "    # run the query against the RAG system\n",
        "    response_rag =\n",
        "    rag_responses.append(str(response_rag))\n",
        "\n",
        "    # get the texts of the nodes that were retrieved for this query as a list\n",
        "    retrieved_node_texts.append(\n",
        "        [### WRITE YOUR CODE HERE ###]\n",
        "    )\n",
        "\n",
        "    # get the scores of the texts of the retrieved nodes as a list\n",
        "    retrieved_node_scores.append(\n",
        "        [### WRITE YOUR CODE HERE ###]\n",
        "    )\n",
        "    ### YOUR CODE HERE ###\n",
        "    # implement the \"vanilla\" recipe generator, which simply prompts the base LLM (e.g., Qwen) rather than using Qwen+RAG\n",
        "    input_text = completion_to_prompt(query)\n",
        "    input_ids = tokenizer.encode(###WRITE YOUR CODE HERE###).to(device)\n",
        "    output = model.generate(\n",
        "          ### WRITE YOUR CODE HERE ###\n",
        "      )\n",
        "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    vanilla_responses.append(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiC74uS87ks2"
      },
      "outputs": [],
      "source": [
        "retrieved_node_scores\n",
        "test_queries#[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to answer the questions below, we'll need to examine the output from the RAG and vanilla systems"
      ],
      "metadata": {
        "id": "z-s6jWxZIaaw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eT4wfTL2grH"
      },
      "outputs": [],
      "source": [
        "print(\"RAG system results with similarity scores ....\")\n",
        "\n",
        "for i, (texts, scores) in enumerate(zip(retrieved_node_texts, retrieved_node_scores)):\n",
        "    print(\"---\" * 100)\n",
        "    print(f\"----- Query {i} -----\" )\n",
        "    print(test_queries[i])\n",
        "    for j, (text, score) in enumerate(zip(texts, scores)):\n",
        "      print(f\"----- Node {j} -----\")\n",
        "      print(text)\n",
        "      print(\"score\")\n",
        "      print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHzqCrYlPW6w"
      },
      "outputs": [],
      "source": [
        "print(\"RAG and vanilla system responses to each query...\")\n",
        "for i, (rag_response, vanilla_response) in enumerate(zip(rag_responses, vanilla_responses)):\n",
        "  print(\"---\" * 100)\n",
        "  print(f\"----- Query {i} -----\" )\n",
        "  print(test_queries[i])\n",
        "  print(\"----- RAG Response -----\")\n",
        "  print(rag_response)\n",
        "  print(\"----- Vanilla Response -----\")\n",
        "  print(vanilla_response.replace(test_queries[i], \"\").strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovKkbasV7ks2"
      },
      "source": [
        "### Questions\n",
        "\n",
        "For questions 1 and 2, first qualitatively compare the output of the vanilla and RAG-based systems.\n",
        "\n",
        "\n",
        "1. Do you observe differences between the quality of the RAG and vanilla responses? If yes, what are these?\n",
        "\n",
        "\n",
        "2.  Inspect the retrieved recipes and their scores. Do they make sense for the queries? Do the scores match your intuition about their relevance for the query?\n",
        "\n",
        "\n",
        "3. Was there any benefit to using RAG for this use-case or could we have just used the vanilla system? Are there other use cases where we'd need to use RAG rather than relying on the base weights of an LLM?\n",
        "\n",
        "\n",
        "4.  What does the embedding model do? What is the measure used to score the relevance of retrieved documents?\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}