{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Class 2: Early text classification\n",
        "\n",
        "**Topics**\n",
        "\n",
        "* Naive Bayes\n",
        "* Logistic Regression\n",
        "* The bag-of-words vector representation\n",
        "\n",
        "**Reading**\n",
        "* [Jurafsky \\& Martin Chapter 4: Naive Bayes, Text Classification, and Sentiment](https://web.stanford.edu/~jurafsky/slp3/4.pdf)\n",
        "* [Jurafsky \\& Martin Chapter 5: Logistic Regression](https://web.stanford.edu/~jurafsky/slp3/5.pdf)\n"
      ],
      "metadata": {
        "id": "8F0z3dlKT2A6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Basic sentiment analysis of movie reviews using Naive Bayes\n",
        "\n",
        "In the next few cells, we'll implement basic sentiment analysis of movie reviews using Naive Bayes and the TF-IDF-weighed bag-of-words feature representation. We'll then examine the top-ranked features by class learned by the classifier"
      ],
      "metadata": {
        "id": "AoS5yWgoU6K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "# Load the movie reviews dataset\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Shuffle the documents\n",
        "import random\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Convert list of words back to strings for TF-IDF Vectorizer\n",
        "document_texts = [\" \".join(d) for (d, c) in documents]\n",
        "document_sentiments = [c for (d, c) in documents]\n",
        "\n",
        "\" \".join(documents[0][0]), documents[0][1]"
      ],
      "metadata": {
        "id": "71HS5Xo4HGnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3073b9b3"
      },
      "source": [
        "`MultinomialNB` implements the Naive Bayes algorithm for multinomial models.\n",
        "It  learns the likelihood of seeing certain words in positive versus negative reviews and uses this information to predict the sentiment of new reviews.\n",
        "\n",
        "1.  **Assumed Distribution:** It assumes that the features follow a multinomial distribution. In the context of text, this means it considers the counts of words in a document.\n",
        "2.  **Naive Assumption:** It makes the \"naive\" assumption that the features are conditionally independent given the class label. This means that the presence or absence of one word does not affect the presence or absence of another word, given that we know the sentiment (e.g., positive or negative). While this assumption is often violated in real-world text, Naive Bayes still performs surprisingly well in many text classification tasks.\n",
        "3.  **Probability Calculation:** For each class (e.g., positive or negative), the model learns the probability of each feature (word) appearing in documents belonging to that class. This is typically done by counting the occurrences of each word in the training data for each class and applying smoothing to handle words that might not appear in all classes.\n",
        "4.  **Classification:** To classify a new document, the model calculates the probability of the document belonging to each class, based on the learned feature probabilities and the prior probability of each class. It then assigns the document to the class with the highest probability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNGZXCycEpzo"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(document_texts, document_sentiments, test_size=0.25, random_state=42)\n",
        "\n",
        "# Use TfidfVectorizer for feature extraction\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=3000) # You can adjust max_features\n",
        "X_train = tfidf_vectorizer.fit_transform(X_train_text)\n",
        "X_test = tfidf_vectorizer.transform(X_test_text)\n",
        "\n",
        "\n",
        "# Train a Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Get most-informative features\n",
        "\n",
        "Now let's take a look at the most informative features for the `pos` and `neg` classes. In Multinomial Naive Bayes, the log likelihood ratio of a feature for a given class (e.g., positive sentiment) compared to all other classes (e.g., negative sentiment) tells us how much more likely that feature is to appear in the positive class compared to the negative class.\n",
        "\n",
        "A higher positive log likelihood ratio for a feature means that the feature is much more likely to be present in positive reviews than in negative reviews, making it a strong indicator of positive sentiment. Conversely, a large negative log likelihood ratio indicates a feature that is much more likely to be found in negative reviews, making it a strong indicator of negative sentiment.\n",
        "\n",
        "By sorting features based on the absolute value of this log likelihood ratio, we can identify the features that have the strongest association with either the positive or negative class, thus indicating their importance in distinguishing between the sentiments.\n",
        "\n"
      ],
      "metadata": {
        "id": "qkFY_xqzGo6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the log probabilities of features for each class\n",
        "neg_prob_features = classifier.feature_log_prob_[0]\n",
        "pos_prob_features = classifier.feature_log_prob_[1]\n",
        "\n",
        "# Get the feature names from the vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame to compare probabilities\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'neg_prob': neg_prob_features,\n",
        "    'pos_prob': pos_prob_features\n",
        "})\n",
        "\n",
        "# Calculate the difference in probabilities (log likelihood ratio)\n",
        "feature_importance['log_likelihood_ratio'] = pos_prob_features - neg_prob_features\n",
        "\n",
        "# Sort by the absolute value of the log likelihood ratio to find the most informative features\n",
        "feature_importance['abs_log_likelihood_ratio'] = abs(feature_importance['log_likelihood_ratio'])\n",
        "most_informative_features = feature_importance.sort_values(by='abs_log_likelihood_ratio', ascending=False).head(20)\n",
        "\n",
        "print(\"\\nMost Informative Features:\")\n",
        "display(most_informative_features[['feature', 'log_likelihood_ratio']])"
      ],
      "metadata": {
        "id": "dNh95ARnFcFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the most informative features for the positive class\n",
        "most_informative_positive_features = feature_importance.sort_values(by='log_likelihood_ratio', ascending=False).head(20)\n",
        "\n",
        "print(\"\\nMost Informative Features for Positive Class:\")\n",
        "display(most_informative_positive_features[['feature', 'log_likelihood_ratio']])"
      ],
      "metadata": {
        "id": "izM3bv70GP_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_informative_negative_features = feature_importance.sort_values(by='log_likelihood_ratio', ascending=False).tail(20)\n",
        "\n",
        "print(\"\\nMost Informative Features for Negative Class:\")\n",
        "display(most_informative_negative_features[['feature', 'log_likelihood_ratio']])"
      ],
      "metadata": {
        "id": "FMKXvg-hG1SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c1e8b72"
      },
      "source": [
        "## 2. Multiclass classification of articles using Logistic Regression\n",
        "\n",
        "In the next few cells, we'll implement a multi-class text classification model using the 20 Newsgroups dataset -- a commonly used dataset for multiclass classification problems--and Logistic Regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a097318c"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "# Select 4 categories for a 4-class classification setting\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Inspect the dataset\n",
        "print(\"Number of training documents:\", len(newsgroups_train.data))\n",
        "print(\"Number of testing documents:\", len(newsgroups_test.data))\n",
        "print(\"Number of categories:\", len(newsgroups_train.target_names))\n",
        "print(\"Categories:\", newsgroups_train.target_names)\n",
        "print(\"\\nFirst document in training set:\")\n",
        "print(newsgroups_train.data[0])\n",
        "print(\"\\nTarget of the first document:\", newsgroups_train.target[0])\n",
        "print(\"Target name of the first document:\", newsgroups_train.target_names[newsgroups_train.target[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Per-topic distribution in dataset"
      ],
      "metadata": {
        "id": "uclQTn2I-7D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Get the category names\n",
        "category_names_train = newsgroups_train.target_names\n",
        "category_names_test = newsgroups_test.target_names\n",
        "\n",
        "# Count the occurrences of each category in the training data\n",
        "category_counts_train = np.bincount(newsgroups_train.target)\n",
        "\n",
        "# Count the occurrences of each category in the testing data\n",
        "category_counts_test = np.bincount(newsgroups_test.target)\n",
        "\n",
        "# Create a bar plot of the category distribution for the training data\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=category_names_train, y=category_counts_train)\n",
        "plt.title('Distribution of News Categories in Training Data')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate labels for better readability\n",
        "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "plt.show()\n",
        "\n",
        "# Create a bar plot of the category distribution for the test data\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=category_names_test, y=category_counts_test)\n",
        "plt.title('Distribution of News Categories in Testing Data')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate labels for better readability\n",
        "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X87OSN75NuOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "691cc8af"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "# We can adjust parameters like max_features, stop_words, etc.\n",
        "vectorizer = TfidfVectorizer(max_features=3000, stop_words='english')\n",
        "\n",
        "# Fit the vectorizer on the training data and transform the training data\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "# Get the target variables\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# Print the shape of the resulting matrices to verify\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Sigmoid, Softmax, and cross-entropy loss: The core components of logistic regression\n"
      ],
      "metadata": {
        "id": "ffHHhSzt_E-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1 The sigmoid Function\n",
        "\n",
        "The sigmoid function, also known as the logistic function, is a crucial component in binary logistic regression. It's defined as:\n",
        "\n",
        "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
        "\n",
        "where $z$ is the input, typically a linear combination of features and weights: $z = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$.\n",
        "\n",
        "The sigmoid function takes any real-valued input and squashes it to a value between 0 and 1. In binary logistic regression, this output is interpreted as the probability of the input belonging to the positive class (e.g., probability of a review being positive).\n",
        "\n",
        "- If $z$ is large positive, $\\sigma(z)$ approaches 1.\n",
        "- If $z$ is large negative, $\\sigma(z)$ approaches 0.\n",
        "- If $z$ is 0, $\\sigma(z)$ is 0.5.\n",
        "\n",
        "This property makes the sigmoid function ideal for modeling probabilities, as probabilities must be between 0 and 1."
      ],
      "metadata": {
        "id": "vSXqevDLKNTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.2 The softmax function\n",
        "\n",
        "While the sigmoid function is used for binary classification, the **softmax function** is its generalization for multiclass classification. It's used to convert a vector of arbitrary real values into a probability distribution over multiple classes. The softmax function is defined as:\n",
        "\n",
        "$$ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} $$\n",
        "\n",
        "where $z = (z_1, z_2, ..., z_K)$ is the input vector (the output of the linear model for each of the $K$ classes), and $z_i$ is the input for class $i$.\n",
        "\n",
        "The softmax function:\n",
        "- Takes a vector of $K$ real numbers as input.\n",
        "- Exponentiates each element to make them non-negative.\n",
        "- Divides each exponentiated element by the sum of all exponentiated elements, ensuring that the output values sum to 1.\n",
        "\n",
        "The output of the softmax function is a vector of $K$ probabilities, where each element represents the probability of the input belonging to a specific class. This makes it suitable for multiclass classification problems where we want to predict the probability of an instance belonging to each of the available classes."
      ],
      "metadata": {
        "id": "yet2lnlmv8RM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dcabd51"
      },
      "source": [
        "##### 2.2.2.1 Visualizing the Softmax Function\n",
        "\n",
        "Let's visualize this with a simple example. Suppose we have an input vector representing the scores for three classes: [1.0, 2.0, 3.0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cca964f6"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def softmax(z):\n",
        "    \"\"\"Compute softmax scores for each class in z.\"\"\"\n",
        "    exp_z = np.exp(z - np.max(z)) # Subtract max for numerical stability\n",
        "    return exp_z / np.sum(exp_z)\n",
        "\n",
        "# Example input vector (e.g., scores for 3 classes)\n",
        "input_scores = np.array([1.0, 2.0, 3.0])\n",
        "softmax_output = softmax(input_scores)\n",
        "\n",
        "print(\"Input Scores:\", input_scores)\n",
        "print(\"Softmax Output (Probabilities):\", softmax_output)\n",
        "\n",
        "# Visualize the input scores and softmax output\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Plot Input Scores\n",
        "axes[0].bar(range(len(input_scores)), input_scores, color='skyblue')\n",
        "axes[0].set_title('Input Scores')\n",
        "axes[0].set_xlabel('Class')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].set_xticks(range(len(input_scores)))\n",
        "axes[0].set_xticklabels([f'Class {i+1}' for i in range(len(input_scores))])\n",
        "axes[0].grid(axis='y', linestyle='--')\n",
        "\n",
        "# Plot Softmax Output\n",
        "axes[1].bar(range(len(softmax_output)), softmax_output, color='lightcoral')\n",
        "axes[1].set_title('Softmax Output (Probabilities)')\n",
        "axes[1].set_xlabel('Class')\n",
        "axes[1].set_ylabel('Probability')\n",
        "axes[1].set_xticks(range(len(softmax_output)))\n",
        "axes[1].set_xticklabels([f'Class {i+1}' for i in range(len(softmax_output))])\n",
        "axes[1].set_ylim(0, 1) # Probabilities are between 0 and 1\n",
        "axes[1].grid(axis='y', linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.3 The cross-Entropy Loss Function\n",
        "\n",
        "The **cross-entropy loss function**, also known as logarithmic loss, is a standard loss function used in classification problems, particularly with models that output probability distributions like logistic regression and neural networks. It measures the difference between the predicted probability distribution and the true class distribution.\n",
        "\n",
        "For a single instance with $K$ classes, where $y_i$ is a binary indicator (1 if the true class is $i$, 0 otherwise) and $p_i$ is the predicted probability of the instance belonging to class $i$, the cross-entropy loss is defined as:\n",
        "\n",
        "$$ H(y, p) = -\\sum_{i=1}^{K} y_i \\log(p_i) $$\n",
        "\n",
        "In the case of a true class $j$, $y_j=1$ and $y_i=0$ for $i \\neq j$. The formula simplifies to:\n",
        "\n",
        "$$ H(y, p) = -\\log(p_j) $$\n",
        "\n",
        "The goal during training is to minimize this loss. Minimizing $-\\log(p_j)$ is equivalent to maximizing $\\log(p_j)$, which in turn is equivalent to maximizing $p_j$, the predicted probability of the true class.\n",
        "\n",
        "Cross-entropy loss penalizes confident wrong predictions heavily. If the model predicts a low probability for the true class, the $-\\log(p_j)$ term will be large, indicating a high loss. Conversely, if the model predicts a high probability for the true class, the loss will be small. This makes it an effective loss function for training classification models to output well-calibrated probabilities."
      ],
      "metadata": {
        "id": "8L97OQuDwY5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.4 Review of stochastic gradient descent with cross-entropy loss\n",
        "\n",
        "See Jurafsky & Martin 5.6\n",
        "\n",
        "Our goal is to find the set of weights which minimizes this loss function, averaged over all examples. Gradient descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters θ) the function’s slope is rising the most steeply, and moving in the opposite direction.\n",
        "\n",
        "The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom, you might look around yourself in all directions, find the direction where the ground is sloping the steepest, and walk downhill in that direction.\n",
        "\n",
        "For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima)\n",
        "\n",
        "Below is a simplified example of Cross-Entropy Loss and SGD.vWe assume a simple binary classification scenario with one feature (x) and we are trying to learn a single weight (w) with no bias for simplicity. The prediction is sigmoid(w * x)\n",
        "\n"
      ],
      "metadata": {
        "id": "weRFefnLAdms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# --- Simplified Example with Cross-Entropy Loss and SGD ---\n",
        "\n",
        "# Let's assume a simple binary classification scenario with one feature (x)\n",
        "# and we are trying to learn a single weight (w) with no bias for simplicity.\n",
        "# The prediction is sigmoid(w * x)\n",
        "\n",
        "# Example Data (Feature x and True Label y)\n",
        "# Data point 1: x=1, y=1 (Positive class)\n",
        "# Data point 2: x=2, y=0 (Negative class)\n",
        "X = np.array([1.0, 2.0])\n",
        "y_true = np.array([1.0, 0.0]) # True labels\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Cross-Entropy Loss function for a single data point\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    # Avoid log(0) by clipping predictions\n",
        "    epsilon = 1e-15\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "\n",
        "# Total Cross-Entropy Loss over all data points\n",
        "def total_cross_entropy_loss(w, X, y_true):\n",
        "    total_loss = 0\n",
        "    for i in range(len(X)):\n",
        "        z = w * X[i]\n",
        "        y_pred = sigmoid(z)\n",
        "        total_loss += cross_entropy_loss(y_true[i], y_pred)\n",
        "    return total_loss / len(X) # Average loss\n",
        "\n",
        "# Gradient of the Cross-Entropy Loss with respect to the weight (w) for a single data point\n",
        "# For sigmoid(wx), the derivative of loss with respect to w is (y_pred - y_true) * x\n",
        "def gradient_cross_entropy(w, x, y_true):\n",
        "    y_pred = sigmoid(w * x)\n",
        "    return (y_pred - y_true) * x\n",
        "\n",
        "# Simulate Stochastic Gradient Descent\n",
        "def stochastic_gradient_descent_ce(initial_w, learning_rate, n_iterations, X, y_true):\n",
        "    w_values = [initial_w]\n",
        "    loss_values = [total_cross_entropy_loss(initial_w, X, y_true)]\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        # In SGD, we pick one data point (or a mini-batch) randomly\n",
        "        # For this simple illustration, let's cycle through the data points\n",
        "        data_index = i % len(X)\n",
        "        x_i = X[data_index]\n",
        "        y_true_i = y_true[data_index]\n",
        "\n",
        "        # Calculate gradient using a single data point\n",
        "        grad = gradient_cross_entropy(w_values[-1], x_i, y_true_i)\n",
        "\n",
        "        # Update the weight\n",
        "        new_w = w_values[-1] - learning_rate * grad\n",
        "        w_values.append(new_w)\n",
        "        loss_values.append(total_cross_entropy_loss(new_w, X, y_true)) # Calculate total loss for visualization\n",
        "\n",
        "    return w_values, loss_values\n",
        "\n",
        "# Set parameters for SGD\n",
        "initial_w = -2.0 # Initial weight\n",
        "learning_rate = 0.5 # Learning rate\n",
        "n_iterations = 300 # Number of iterations\n",
        "\n",
        "# Run SGD simulation\n",
        "w_steps, loss_steps = stochastic_gradient_descent_ce(initial_w, learning_rate, n_iterations, X, y_true)\n",
        "\n",
        "# Generate w values for plotting the total loss function\n",
        "w_plot = np.linspace(-3, 4, 100)\n",
        "loss_plot = [total_cross_entropy_loss(w, X, y_true) for w in w_plot]\n",
        "\n",
        "# Create interactive plot using Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the loss function curve\n",
        "fig.add_trace(go.Scatter(x=w_plot, y=loss_plot,\n",
        "                         mode='lines',\n",
        "                         name='Average Cross-Entropy Loss'))\n",
        "\n",
        "# Add the SGD steps as points with hover information\n",
        "fig.add_trace(go.Scatter(x=w_steps, y=loss_steps,\n",
        "                         mode='markers+lines',\n",
        "                         name='SGD Steps',\n",
        "                         marker=dict(size=8),\n",
        "                         hovertemplate='Weight: %{x:.4f}<br>Loss: %{y:.4f}<extra></extra>')) # Customize hover info\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='Illustration of Stochastic Gradient Descent with Cross-Entropy Loss',\n",
        "    xaxis_title='Weight Value (w)',\n",
        "    yaxis_title='Average Loss',\n",
        "    hovermode='x unified' # Show hover info for all traces at a given x-value\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0Ue4ghWlL7q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d211da4"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Multinomial Logistic Regression model\n",
        "# We use the 'sag' solver which is suitable for large datasets and supports stochastic gradient descent.\n",
        "# 'multi_class' is not needed with 'sag' as it defaults to 'auto' which handles multinomial classification.\n",
        "model = LogisticRegression(solver='sag', random_state=42, n_jobs=-1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4785af67"
      },
      "source": [
        "### 2.4 Evaluating the model\n",
        " We will use common classification metrics like accuracy, precision, recall, and F1-score.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "3VPTDvtuyfs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "MBbUcqvOy9uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db99901e"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=newsgroups_test.target_names)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "kBgNL6_pOo-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91743c8d"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=newsgroups_test.target_names, yticklabels=newsgroups_test.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sOIdf-hODW6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Examine the most informative per-class features\n",
        "\n",
        "To determine the features that provided the best discriminative signal, we can simply look at the model coefficients."
      ],
      "metadata": {
        "id": "4i_oGaYBDY8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model coefficients\n",
        "# The coefficients are stored in the coef_ attribute of the LogisticRegression model\n",
        "# coef_ is an array of shape (n_classes, n_features)\n",
        "coefficients = model.coef_\n",
        "\n",
        "# Get the feature names from the vectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Get the class names\n",
        "class_names = newsgroups_train.target_names\n",
        "\n",
        "print(\"Most Informative Features for Each Class (Logistic Regression):\")\n",
        "\n",
        "# Iterate through each class to find the top features\n",
        "for i, class_name in enumerate(class_names):\n",
        "    # Get the coefficients for the current class\n",
        "    class_coefficients = coefficients[i]\n",
        "\n",
        "    # Create a DataFrame to associate features with their coefficients for this class\n",
        "    feature_coefficient_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'coefficient': class_coefficients\n",
        "    })\n",
        "\n",
        "    # Sort features by the absolute value of their coefficients to find the most important\n",
        "    feature_coefficient_df['abs_coefficient'] = abs(feature_coefficient_df['coefficient'])\n",
        "    most_informative_class_features = feature_coefficient_df.sort_values(by='abs_coefficient', ascending=False).head(20)\n",
        "\n",
        "    print(f\"\\n--- Class: {class_name} ---\")\n",
        "    display(most_informative_class_features[['feature', 'coefficient']])"
      ],
      "metadata": {
        "id": "RPilVwPMDJmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}